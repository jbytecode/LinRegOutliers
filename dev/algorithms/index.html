<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Algorithms · LinRegOutliers</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>LinRegOutliers</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Algorithms</a><ul class="internal"><li><a class="tocitem" href="#Hadi-and-Simonoff-(1993)"><span>Hadi &amp; Simonoff (1993)</span></a></li><li><a class="tocitem" href="#Kianifard-and-Swallow-(1989)"><span>Kianifard &amp; Swallow (1989)</span></a></li><li><a class="tocitem" href="#Sebert-and-Montgomery-and-Rollier-(1998)"><span>Sebert &amp; Montgomery &amp; Rollier (1998)</span></a></li><li><a class="tocitem" href="#Least-Median-of-Squares"><span>Least Median of Squares</span></a></li><li><a class="tocitem" href="#Least-Trimmed-Squares"><span>Least Trimmed Squares</span></a></li><li><a class="tocitem" href="#Minimum-Volume-Ellipsoid-(MVE)"><span>Minimum Volume Ellipsoid (MVE)</span></a></li><li><a class="tocitem" href="#MVE-and-LTS-Plot"><span>MVE &amp; LTS Plot</span></a></li><li><a class="tocitem" href="#Billor-and-Chatterjee-and-Hadi-(2006)"><span>Billor &amp; Chatterjee &amp; Hadi (2006)</span></a></li><li><a class="tocitem" href="#Pena-and-Yohai-(1995)"><span>Pena &amp; Yohai (1995)</span></a></li><li><a class="tocitem" href="#Satman-(2013)"><span>Satman (2013)</span></a></li><li><a class="tocitem" href="#Satman-(2015)"><span>Satman (2015)</span></a></li><li><a class="tocitem" href="#Least-Absolute-Deviations-(LAD)"><span>Least Absolute Deviations (LAD)</span></a></li><li><a class="tocitem" href="#Least-Trimmed-Absolute-Deviations-(LTA)"><span>Least Trimmed Absolute Deviations (LTA)</span></a></li><li><a class="tocitem" href="#Hadi-(1992)"><span>Hadi (1992)</span></a></li><li><a class="tocitem" href="#Marchette-and-Solka-(2003)-Data-Images"><span>Marchette &amp; Solka (2003) Data Images</span></a></li><li><a class="tocitem" href="#Satman&#39;s-GA-based-LTS-estimation-(2012)"><span>Satman&#39;s GA based LTS estimation (2012)</span></a></li><li><a class="tocitem" href="#Fischler-and-Bolles-(1981)-RANSAC-Algorithm"><span>Fischler &amp; Bolles (1981) RANSAC Algorithm</span></a></li><li><a class="tocitem" href="#Minimum-Covariance-Determinant-Estimator-(MCD)"><span>Minimum Covariance Determinant Estimator (MCD)</span></a></li><li><a class="tocitem" href="#Imon-(2005)-Algorithm"><span>Imon (2005) Algorithm</span></a></li><li><a class="tocitem" href="#Barratt-and-Angeris-and-Boyd-(2020)-CCF-algorithm"><span>Barratt &amp; Angeris &amp; Boyd (2020) CCF algorithm</span></a></li><li><a class="tocitem" href="#Atkinson-(1994)-Forward-Search-Algorithm"><span>Atkinson (1994) Forward Search Algorithm</span></a></li><li><a class="tocitem" href="#BACON-Algorithm-(Billor-and-Hadi-and-Velleman-(2000))"><span>BACON Algorithm (Billor &amp; Hadi &amp; Velleman (2000))</span></a></li><li><a class="tocitem" href="#Hadi-(1994)-Algorithm"><span>Hadi (1994) Algorithm</span></a></li><li><a class="tocitem" href="#Chatterjee-and-Mächler-(1997)"><span>Chatterjee &amp; Mächler (1997)</span></a></li></ul></li><li><a class="tocitem" href="../diagnostics/">Diagnostics</a></li><li><a class="tocitem" href="../types/">Types</a></li><li><a class="tocitem" href="../datasets/">Datasets</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Algorithms</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Algorithms</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/jbytecode/LinRegOutliers/blob/master/docs/src/algorithms.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Algorithms"><a class="docs-heading-anchor" href="#Algorithms">Algorithms</a><a id="Algorithms-1"></a><a class="docs-heading-anchor-permalink" href="#Algorithms" title="Permalink"></a></h1><h2 id="Hadi-and-Simonoff-(1993)"><a class="docs-heading-anchor" href="#Hadi-and-Simonoff-(1993)">Hadi &amp; Simonoff (1993)</a><a id="Hadi-and-Simonoff-(1993)-1"></a><a class="docs-heading-anchor-permalink" href="#Hadi-and-Simonoff-(1993)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.HS93.hs93" href="#LinRegOutliers.HS93.hs93"><code>LinRegOutliers.HS93.hs93</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">hs93(setting; alpha = 0.05, basicsubsetindices = nothing)</code></pre><p>Perform the Hadi &amp; Simonoff (1993) algorithm for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li><li><code>alpha::Float64</code>: Optional argument of the probability of rejecting the null hypothesis.</li><li><code>basicsubsetindices::Array{Int, 1}</code>: Initial basic subset, by default, the algorithm creates an initial set of clean observations.</li></ul><p><strong>Description</strong></p><p>Performs a forward search by selecting and enlarging an initial clean subset of observations and  iterates until scaled residuals exceeds a threshold.</p><p><strong>Output</strong></p><ul><li><code>[&quot;outliers&quot;]</code>: Array of indices of outliers</li><li><code>[&quot;t&quot;]</code>: Threshold, specifically, calculated quantile of a Student-T distribution</li><li><code>[&quot;d&quot;]</code>: Internal and external scaled residuals. </li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; reg0001 = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; hs93(reg0001)
Dict{Any,Any} with 3 entries:
  &quot;outliers&quot; =&gt; [14, 15, 16, 17, 18, 19, 20, 21]
  &quot;t&quot;        =&gt; -3.59263
  &quot;d&quot;        =&gt; [2.04474, 1.14495, -0.0633255, 0.0632934, -0.354349, -0.766818, -1.06862, -1.47638, -0.7…</code></pre><p><strong>References</strong></p><p>Hadi, Ali S., and Jeffrey S. Simonoff. &quot;Procedures for the identification of  multiple outliers in linear models.&quot; Journal of the American Statistical  Association 88.424 (1993): 1264-1272.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/hs93.jl#L129-L164">source</a></section></article><h2 id="Kianifard-and-Swallow-(1989)"><a class="docs-heading-anchor" href="#Kianifard-and-Swallow-(1989)">Kianifard &amp; Swallow (1989)</a><a id="Kianifard-and-Swallow-(1989)-1"></a><a class="docs-heading-anchor-permalink" href="#Kianifard-and-Swallow-(1989)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.KS89.ks89" href="#LinRegOutliers.KS89.ks89"><code>LinRegOutliers.KS89.ks89</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">ks89(setting; alpha = 0.05)</code></pre><p>Perform the Kianifard &amp; Swallow (1989) algorithm for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li><li><code>alpha::Float64</code>: Optional argument of the probability of rejecting the null hypothesis.</li></ul><p><strong>Description</strong></p><p>The algorithm starts with a clean subset of observations. This initial set is then enlarged  using recursive residuals. When the calculated statistics exceeds a threshold it terminates. </p><p><strong>Output</strong></p><ul><li><code>[&quot;outliers]</code>: Array of indices of outliers.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; reg0001 = createRegressionSetting(@formula(stackloss ~ airflow + watertemp + acidcond), stackloss)
julia&gt; ks89(reg0001)
Dict{String,Array{Int64,1}} with 1 entry:
  &quot;outliers&quot; =&gt; [4, 21]</code></pre><p><strong>References</strong></p><p>Kianifard, Farid, and William H. Swallow. &quot;Using recursive residuals, calculated on adaptively-ordered observations, to identify outliers in linear regression.&quot; Biometrics (1989): 571-585.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/ks89.jl#L48-L77">source</a></section></article><h2 id="Sebert-and-Montgomery-and-Rollier-(1998)"><a class="docs-heading-anchor" href="#Sebert-and-Montgomery-and-Rollier-(1998)">Sebert &amp; Montgomery &amp; Rollier (1998)</a><a id="Sebert-and-Montgomery-and-Rollier-(1998)-1"></a><a class="docs-heading-anchor-permalink" href="#Sebert-and-Montgomery-and-Rollier-(1998)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.SMR98.smr98" href="#LinRegOutliers.SMR98.smr98"><code>LinRegOutliers.SMR98.smr98</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">smr98(setting)</code></pre><p>Perform the Sebert, Monthomery and Rollier (1998) algorithm for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li></ul><p><strong>Description</strong></p><p>The algorithm starts with an ordinary least squares  estimation for a given model and data. Residuals and fitted responses are calculated  using the estimated model. A hierarchical clustering analysis is applied using standardized residuals and standardized fitted responses. The tree structure of clusters are cut using a threshold, e.g Majona criterion, as suggested by the authors. It is expected that  the subtrees with relatively small number of observations are declared to be clusters of outliers.</p><p><strong>Output</strong></p><ul><li><code>[&quot;outliers&quot;]</code>: Array of indices of outliers.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; reg0001 = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; smr98(reg0001)
Dict{String,Array{Int64,1}} with 1 entry:
  &quot;outliers&quot; =&gt; [15, 16, 17, 18, 19, 20, 21, 22, 23, 24]</code></pre><p><strong>References</strong></p><p>Sebert, David M., Douglas C. Montgomery, and Dwayne A. Rollier. &quot;A clustering algorithm for  identifying multiple outliers in linear regression.&quot; Computational statistics &amp; data analysis  27.4 (1998): 461-484.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/smr98.jl#L33-L65">source</a></section></article><h2 id="Least-Median-of-Squares"><a class="docs-heading-anchor" href="#Least-Median-of-Squares">Least Median of Squares</a><a id="Least-Median-of-Squares-1"></a><a class="docs-heading-anchor-permalink" href="#Least-Median-of-Squares" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.LMS.lms" href="#LinRegOutliers.LMS.lms"><code>LinRegOutliers.LMS.lms</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">lms(setting; iters = nothing, crit = 2.5)</code></pre><p>Perform Least Median of Squares regression estimator with random sampling.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: A regression setting object.</li><li><code>iters::Int</code>: Number of random samples.</li><li><code>crit::Float64</code>: Critical value for standardized residuals. </li></ul><p><strong>Description</strong></p><p>LMS (Least Median of Squares) estimator is highly robust with 50% breakdown property. The algorithm searches for regression coefficients which minimize (h)th ordered squared residual where h is Int(floor((n + 1.0) / 2.0))</p><p><strong>Output</strong></p><ul><li><code>[&quot;stdres&quot;]</code>: Array of standardized residuals</li><li><code>[&quot;S&quot;]</code>: Standard error of regression</li><li><code>[&quot;outliers&quot;]</code>: Array of indices of outliers</li><li><code>[&quot;objective&quot;]</code>: LMS objective value</li><li><code>[&quot;coef&quot;]</code>: Estimated regression coefficients</li><li><code>[&quot;crit&quot;]</code>: Threshold value.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; reg = createRegressionSetting(@formula(calls ~ year), phones);

julia&gt; lms(reg)
Dict{Any,Any} with 6 entries:
  &quot;stdres&quot;    =&gt; [2.28328, 1.55551, 0.573308, 0.608843, 0.220321, -0.168202, -0.471913, -0.860435, -0.31603, -0.110871  …  85.7265, 88.9849, 103.269, 116.705, 135.229, 159.69,…
  &quot;S&quot;         =&gt; 1.17908
  &quot;outliers&quot;  =&gt; [14, 15, 16, 17, 18, 19, 20, 21]
  &quot;objective&quot; =&gt; 0.515348
  &quot;coef&quot;      =&gt; [-56.1972, 1.1581]
  &quot;crit&quot;      =&gt; 2.5</code></pre><p><strong>References</strong></p><p>Rousseeuw, Peter J. &quot;Least median of squares regression.&quot; Journal of the American  statistical association 79.388 (1984): 871-880.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/lms.jl#L14-L57">source</a></section></article><h2 id="Least-Trimmed-Squares"><a class="docs-heading-anchor" href="#Least-Trimmed-Squares">Least Trimmed Squares</a><a id="Least-Trimmed-Squares-1"></a><a class="docs-heading-anchor-permalink" href="#Least-Trimmed-Squares" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.LTS.lts" href="#LinRegOutliers.LTS.lts"><code>LinRegOutliers.LTS.lts</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">lts(setting; iters, crit)</code></pre><p>Perform the Fast-LTS (Least Trimmed Squares) algorithm for a given regression setting. </p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li><li><code>iters::Int</code>: Number of iterations.</li><li><code>crit::Float64</code>: Critical value.</li></ul><p><strong>Description</strong></p><p>The algorithm searches for estimations of regression parameters which minimize the sum of first h  ordered squared residuals where h is Int(floor((n + p + 1.0) / 2.0)). Specifically, our implementation,  uses the algorithm Fast-LTS in which concentration steps are used for enlarging a basic  subset to subset of clean observation of size h.  </p><p><strong>Output</strong></p><ul><li><code>[&quot;betas&quot;]</code>: Estimated regression coefficients</li><li><code>[&quot;S&quot;]</code>: Standard error of regression</li><li><code>[&quot;hsubset&quot;]</code>: Best subset of clean observation of size h.</li><li><code>[&quot;outliers&quot;]</code>: Array of indices of outliers</li><li><code>[&quot;scaled.residuals&quot;]</code>: Array of scaled residuals</li><li><code>[&quot;objective&quot;]</code>: LTS objective value.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; reg = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; lts(reg)
Dict{Any,Any} with 6 entries:
  &quot;betas&quot;            =&gt; [-56.5219, 1.16488]
  &quot;S&quot;                =&gt; 1.10918
  &quot;hsubset&quot;          =&gt; [11, 10, 5, 6, 23, 12, 13, 9, 24, 7, 3, 4, 8]
  &quot;outliers&quot;         =&gt; [14, 15, 16, 17, 18, 19, 20, 21]
  &quot;scaled.residuals&quot; =&gt; [2.41447, 1.63472, 0.584504, 0.61617, 0.197052, -0.222066, -0.551027, -0.970146, -0.397538, -0.185558  …  …
  &quot;objective&quot;        =&gt; 3.43133</code></pre><p><strong>References</strong></p><p>Rousseeuw, Peter J., and Katrien Van Driessen. &quot;An algorithm for positive-breakdown  regression based on concentration steps.&quot; Data Analysis.  Springer, Berlin, Heidelberg, 2000. 335-346.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/lts.jl#L86-L130">source</a></section></article><h2 id="Minimum-Volume-Ellipsoid-(MVE)"><a class="docs-heading-anchor" href="#Minimum-Volume-Ellipsoid-(MVE)">Minimum Volume Ellipsoid (MVE)</a><a id="Minimum-Volume-Ellipsoid-(MVE)-1"></a><a class="docs-heading-anchor-permalink" href="#Minimum-Volume-Ellipsoid-(MVE)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.MVE.mve" href="#LinRegOutliers.MVE.mve"><code>LinRegOutliers.MVE.mve</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">mve(data; alpha = 0.01)</code></pre><p>Performs the Minimum Volume Ellipsoid algorithm for a robust covariance matrix.</p><p><strong>Arguments</strong></p><ul><li><code>data::DataFrame</code>: Multivariate data.</li><li><code>alpha::Float64</code>: Probability for quantiles of Chi-Squared statistic.</li></ul><p><strong>Description</strong></p><p><code>mve</code> searches for a robust location vector and a robust scale matrix, e.g covariance matrix. The method also reports a usable diagnostic measure, Mahalanobis distances, which are calculated using  the robust counterparts instead of mean vector and usual covariance matrix. Mahalanobis distances  are directly comparible with quantiles of a ChiSquare Distribution with <code>p</code> degrees of freedom.</p><p><strong>Output</strong></p><ul><li><code>[&quot;goal&quot;]</code>: Objective value</li><li><code>[&quot;best.subset&quot;]</code>: Indices of best h-subset of observations</li><li><code>[&quot;robust.location&quot;]</code>: Vector of robust location measures</li><li><code>[&quot;robust.covariance&quot;]</code>: Robust covariance matrix</li><li><code>[&quot;squared.mahalanobis&quot;]</code>: Array of Mahalanobis distances calculated using robust location and scale measures.</li><li><code>[&quot;chisq.crit&quot;]</code>: Chisquare quantile used in threshold</li><li><code>[&quot;alpha&quot;]</code>: Probability used in calculating the Chisquare quantile, e.g <code>chisq.crit</code></li><li><code>[&quot;outliers&quot;]</code>: Array of indices of outliers.</li></ul><p><strong>References</strong></p><p>Van Aelst, Stefan, and Peter Rousseeuw. &quot;Minimum volume ellipsoid.&quot; Wiley  Interdisciplinary Reviews: Computational Statistics 1.1 (2009): 71-82.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/mve.jl#L85-L115">source</a></section></article><h2 id="MVE-and-LTS-Plot"><a class="docs-heading-anchor" href="#MVE-and-LTS-Plot">MVE &amp; LTS Plot</a><a id="MVE-and-LTS-Plot-1"></a><a class="docs-heading-anchor-permalink" href="#MVE-and-LTS-Plot" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.MVELTSPlot.mveltsplot" href="#LinRegOutliers.MVELTSPlot.mveltsplot"><code>LinRegOutliers.MVELTSPlot.mveltsplot</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">mveltsplot(setting; alpha = 0.05, showplot = true)</code></pre><p>Generate MVE - LTS plot for visual detecting of regression outliers.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: A regression setting object.</li><li><code>alpha::Float64</code>: Probability for quantiles of Chi-Squared statistic.</li><li><code>showplot::Bool</code>: Whether a plot is shown or only return statistics.</li></ul><p><strong>Description</strong></p><p>This is a method of combination of <code>lts</code> and <code>mve</code>. Regression residuals and robust distances obtained  by <code>mve</code> and <code>mve</code> are used to generate a plot. Despite this is a visual method, drawing a plot is not really necessary. The algorithm divides the residuals-distances space into 4 parts, one for clean observations, one for vertical outliers (y-space outliers), one for bad-leverage points (x-space outliers), and one for  good leverage points (observations far from the remaining of data in both x and y space).   </p><p><strong>Output</strong></p><ul><li><code>[&quot;plot&quot;]</code>: Generated plot object</li><li><code>[&quot;robust.distances&quot;]</code>: Robust Mahalanobis distances </li><li><code>[&quot;scaled.residuals&quot;]</code>: Scaled residuals of an <code>lts</code> estimate</li><li><code>[&quot;chi.squared&quot;]</code>: Quantile of Chi-Squared distribution </li><li><code>[&quot;regular.points&quot;]</code>: Array of indices of clean observations</li><li><code>[&quot;outlier.points&quot;]</code>: Array of indices of y-space outliers (vertical outliers)</li><li><code>[&quot;leverage.points&quot;]</code>: Array of indices of x-space outliers (bad leverage points)</li><li><code>[&quot;outlier.and.leverage.points&quot;]</code>: Array of indices of xy-space outliers (good leverage points)</li></ul><p><strong>References</strong></p><p>Van Aelst, Stefan, and Peter Rousseeuw. &quot;Minimum volume ellipsoid.&quot; Wiley  Interdisciplinary Reviews: Computational Statistics 1.1 (2009): 71-82.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/mveltsplot.jl#L14-L44">source</a></section></article><h2 id="Billor-and-Chatterjee-and-Hadi-(2006)"><a class="docs-heading-anchor" href="#Billor-and-Chatterjee-and-Hadi-(2006)">Billor &amp; Chatterjee &amp; Hadi (2006)</a><a id="Billor-and-Chatterjee-and-Hadi-(2006)-1"></a><a class="docs-heading-anchor-permalink" href="#Billor-and-Chatterjee-and-Hadi-(2006)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.BCH.bch" href="#LinRegOutliers.BCH.bch"><code>LinRegOutliers.BCH.bch</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">bch(setting; alpha = 0.05, maxiter = 1000, epsilon = 0.000001)</code></pre><p>Perform the Billor &amp; Chatterjee &amp; Hadi (2006) algorithm for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li><li><code>alpha::Float64</code>: Optional argument of the probability of rejecting the null hypothesis.</li><li><code>maxiter::Int</code>: Maximum number of iterations for calculating iterative weighted least squares estimates.</li><li><code>epsilon::Float64</code>: Accuracy for determining convergency.</li></ul><p><strong>Description</strong></p><p>The algorithm initially constructs a basic subset. These basic subset is then used to  generate initial weights for a iteratively least squares estimation. Regression coefficients obtained  in this stage are robust regression estimates. Squared normalized distances and squared normalized  residuals are used in <code>bchplot</code> which serves a visual way for investigation of outliers and their  properties.</p><p><strong>Output</strong></p><ul><li><code>[&quot;betas&quot;]</code>: Final estimate of regression coefficients                               </li><li><code>[&quot;squared.normalized.robust.distances&quot;]</code>:  </li><li><code>[&quot;weights&quot;]</code>: Final weights used in calculation of WLS estimates                             </li><li><code>[&quot;outliers&quot;]</code>: Array of indices of outliers</li><li><code>[&quot;squared.normalized.residuals&quot;]</code>: Array of squared normalized residuals</li><li><code>[&quot;residuals&quot;]</code>: Array of regression residuals</li><li><code>[&quot;basic.subset&quot;]</code>: Array of indices of basic subset.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; reg  = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; Dict{Any,Any} with 7 entries:
&quot;betas&quot;                               =&gt; [-55.9205, 1.15572]
&quot;squared.normalized.robust.distances&quot; =&gt; [0.104671, 0.0865052, 0.0700692, 0.0553633, 0.0423875, 0.03…
&quot;weights&quot;                             =&gt; [0.00186158, 0.00952088, 0.0787321, 0.0787321, 0.0787321, 0…
&quot;outliers&quot;                            =&gt; [1, 14, 15, 16, 17, 18, 19, 20, 21]
&quot;squared.normalized.residuals&quot;        =&gt; [5.53742e-5, 2.42977e-5, 2.36066e-6, 2.77706e-6, 1.07985e-7…
&quot;residuals&quot;                           =&gt; [2.5348, 1.67908, 0.523367, 0.567651, 0.111936, -0.343779, …
&quot;basic.subset&quot;                        =&gt; [1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  15, 16, 17, 18, 19, 20, …</code></pre><p><strong>References</strong></p><p>Billor, Nedret, Samprit Chatterjee, and Ali S. Hadi. &quot;A re-weighted least squares method  for robust regression estimation.&quot; American journal of mathematical and management sciences 26.3-4 (2006): 229-252.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/bch.jl#L19-L65">source</a></section></article><h2 id="Pena-and-Yohai-(1995)"><a class="docs-heading-anchor" href="#Pena-and-Yohai-(1995)">Pena &amp; Yohai (1995)</a><a id="Pena-and-Yohai-(1995)-1"></a><a class="docs-heading-anchor-permalink" href="#Pena-and-Yohai-(1995)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.PY95.py95" href="#LinRegOutliers.PY95.py95"><code>LinRegOutliers.PY95.py95</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">py95(setting)</code></pre><p>Perform the Pena &amp; Yohai (1995) algorithm for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li></ul><p><strong>Description</strong></p><p>The algorithm starts by constructing an influence matrix using results  of an ordinary least squares estimate for a given model and data. In the second stage,  the eigen structure of the influence matrix is examined for detecting suspected subsets of data.</p><p><strong>Output</strong></p><ul><li><code>[&quot;outliers&quot;]</code>: Array of indices of outliers</li><li><code>[&quot;suspected.sets&quot;]</code>: Arrays of indices of observations for corresponding eigen value of the influence matrix.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; reg0001 = createRegressionSetting(@formula(y ~ x1 + x2 + x3), hbk);
julia&gt; py95(reg0001)
ict{Any,Any} with 2 entries:
  &quot;outliers&quot;       =&gt; [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
  &quot;suspected.sets&quot; =&gt; Set([[14, 13], [43, 54, 24, 38, 22], [6, 10], [14, 7, 8, 3, 10, 2, 5, 6, 1, 9, 4…</code></pre><p><strong>References</strong></p><p>Peña, Daniel, and Victor J. Yohai. &quot;The detection of influential subsets in linear  regression by using an influence matrix.&quot; Journal of the Royal Statistical Society:  Series B (Methodological) 57.1 (1995): 145-156.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/py95.jl#L155-L187">source</a></section></article><h2 id="Satman-(2013)"><a class="docs-heading-anchor" href="#Satman-(2013)">Satman (2013)</a><a id="Satman-(2013)-1"></a><a class="docs-heading-anchor-permalink" href="#Satman-(2013)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.Satman2013.satman2013" href="#LinRegOutliers.Satman2013.satman2013"><code>LinRegOutliers.Satman2013.satman2013</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">satman2013(setting)</code></pre><p>Perform Satman (2013) algorithm for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li></ul><p><strong>Description</strong></p><p>The algorithm constructs a fast and robust covariance matrix to calculate robust mahalanobis distances. These distances are then used to construct weights for later use in a weighted least  squares estimation. In the last stage, C-steps are iterated on the basic subset found in previous stages. </p><p><strong>Output</strong></p><ul><li><code>[&quot;outliers&quot;]</code>: Array of indices of outliers.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; eg0001 = createRegressionSetting(@formula(y ~ x1 + x2 + x3), hbk);
julia&gt; satman2013(reg0001)
Dict{Any,Any} with 1 entry:
  &quot;outliers&quot; =&gt; [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 47]
</code></pre><p><strong>References</strong></p><p>Satman, Mehmet Hakan. &quot;A new algorithm for detecting outliers in linear regression.&quot;  International Journal of statistics and Probability 2.3 (2013): 101.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/satman2013.jl#L15-L45">source</a></section></article><h2 id="Satman-(2015)"><a class="docs-heading-anchor" href="#Satman-(2015)">Satman (2015)</a><a id="Satman-(2015)-1"></a><a class="docs-heading-anchor-permalink" href="#Satman-(2015)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.Satman2015.satman2015" href="#LinRegOutliers.Satman2015.satman2015"><code>LinRegOutliers.Satman2015.satman2015</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">satman2013(setting)</code></pre><p>Perform Satman (2015) algorithm for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li></ul><p><strong>Description</strong></p><p>The algorithm starts with sorting the design matrix using the Non-dominated sorting algorithm. An initial basic subset is then constructed using the ranks obtained in previous stage. After many  C-steps, observations with high standardized residuals are reported to be outliers.</p><p><strong>Output</strong></p><ul><li><code>[&quot;outliers]</code>&quot;: Array of indices of outliers.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; eg0001 = createRegressionSetting(@formula(y ~ x1 + x2 + x3), hbk);
julia&gt; satman2013(reg0001)
Dict{Any,Any} with 1 entry:
  &quot;outliers&quot; =&gt; [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 14, 47]
</code></pre><p><strong>References</strong></p><p>Satman, Mehmet Hakan. &quot;A new algorithm for detecting outliers in linear regression.&quot;  International Journal of statistics and Probability 2.3 (2013): 101.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/satman2015.jl#L212-L242">source</a></section></article><p>## Setan &amp; Halim &amp; Mohd (2000)</p><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.ASM2000.asm2000" href="#LinRegOutliers.ASM2000.asm2000"><code>LinRegOutliers.ASM2000.asm2000</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">asm2000(setting)</code></pre><p>Perform the Setan, Halim and Mohd (2000) algorithm for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li></ul><p><strong>Description</strong></p><p>The algorithm performs a Least Trimmed Squares (LTS) estimate and yields standardized  residual - fitted response pairs. A single linkage clustering algorithm is performed on these pairs. Like <code>smr98</code>, the cluster tree is cut using the Majona criterion. Subtrees with  relatively small number of observations are declared to be outliers.</p><p><strong>Output</strong></p><ul><li><code>[&quot;outliers&quot;]</code>: Vector of indices of outliers.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; reg0001 = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; asm2000(reg0001)
Dict{Any,Any} with 1 entry:
  &quot;outliers&quot; =&gt; [15, 16, 17, 18, 19, 20]</code></pre><p><strong>References</strong></p><p>Setan, Halim, and Mohd Nor Mohamad. &quot;Identifying multiple outliers in  linear regression: Robust fit and clustering approach.&quot; (2000).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/asm2000.jl#L17-L47">source</a></section></article><h2 id="Least-Absolute-Deviations-(LAD)"><a class="docs-heading-anchor" href="#Least-Absolute-Deviations-(LAD)">Least Absolute Deviations (LAD)</a><a id="Least-Absolute-Deviations-(LAD)-1"></a><a class="docs-heading-anchor-permalink" href="#Least-Absolute-Deviations-(LAD)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.LAD.lad" href="#LinRegOutliers.LAD.lad"><code>LinRegOutliers.LAD.lad</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">lad(setting)</code></pre><p>Perform Least Absolute Deviations regression for a given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li></ul><p><strong>Description</strong></p><p>The LAD estimator searches for regression parameters estimates that minimizes the sum of absolute residuals. The optimization problem is </p><p>Min z = u1(-) + u1(+) + u2(-) + u2(+) + .... + un(-) + un(+) Subject to:     y<em>1 - beta0 - beta1 * x</em>2 + u1(-) - u1(+) = 0     y<em>2 - beta0 - beta1 * x</em>2 + u2(-) - u2(+) = 0     .     .     .     y<em>n - beta0 - beta1 * x</em>n + un(-) - un(+) = 0 where      ui(-), ui(+) &gt;= 0     i = 1, 2, ..., n      beta0, beta1 in R      n : Number of observations </p><p><strong>Output</strong></p><ul><li><code>[&quot;betas&quot;]</code>: Estimated regression coefficients</li><li><code>[&quot;residuals&quot;]</code>: Regression residuals</li><li><code>[&quot;model&quot;]</code>: Linear Programming Model</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; reg0001 = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; lad(reg0001)
Dict{Any,Any} with 2 entries:
  &quot;betas&quot;     =&gt; [-57.3269, 1.19155]
  &quot;residuals&quot; =&gt; [2.14958, 1.25803, 0.0664872, 0.0749413, -0.416605, -0.90815, -1.2997, -1.79124,…
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/lad.jl#L10-L52">source</a></section><section><div><pre><code class="nohighlight hljs">lad(X, y)</code></pre><p>Perform Least Absolute Deviations regression for a given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>X::Array{Float64, 2}</code>: Design matrix of the linear model.</li><li><code>y::Array{Float64, 1}</code>: Response vector of the linear model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/lad.jl#L59-L68">source</a></section></article><h2 id="Least-Trimmed-Absolute-Deviations-(LTA)"><a class="docs-heading-anchor" href="#Least-Trimmed-Absolute-Deviations-(LTA)">Least Trimmed Absolute Deviations (LTA)</a><a id="Least-Trimmed-Absolute-Deviations-(LTA)-1"></a><a class="docs-heading-anchor-permalink" href="#Least-Trimmed-Absolute-Deviations-(LTA)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.LTA.lta" href="#LinRegOutliers.LTA.lta"><code>LinRegOutliers.LTA.lta</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">lta(setting; exact = false)</code></pre><p>Perform the Hawkins &amp; Olive (1999) algorithm (Least Trimmed Absolute Deviations)  for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li><li><code>exact::Bool</code>: Consider all possible subsets of p or not where p is the number of regression parameters.</li></ul><p><strong>Description</strong></p><p><code>lta</code> is a trimmed version of <code>lad</code> in which the sum of first h absolute residuals is minimized where h is Int(floor((n + p + 1.0) / 2.0)). </p><p><strong>Output</strong></p><ul><li><code>[&quot;betas&quot;]</code>: Estimated regression coefficients</li><li><code>[&quot;objective]</code>: Objective value</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; reg0001 = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; lta(reg0001)
Dict{Any,Any} with 2 entries:
  &quot;betas&quot;     =&gt; [-55.5, 1.15]
  &quot;objective&quot; =&gt; 5.7

julia&gt; lta(reg0001, exact = true)
Dict{Any,Any} with 2 entries:
  &quot;betas&quot;     =&gt; [-55.5, 1.15]
  &quot;objective&quot; =&gt; 5.7  </code></pre><p><strong>References</strong></p><p>Hawkins, Douglas M., and David Olive. &quot;Applications and algorithms for least trimmed sum of  absolute deviations regression.&quot; Computational Statistics &amp; Data Analysis 32.2 (1999): 119-134.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/lta.jl#L13-L52">source</a></section><section><div><pre><code class="nohighlight hljs">lta(X, y; exact = false)</code></pre><p>Perform the Hawkins &amp; Olive (1999) algorithm (Least Trimmed Absolute Deviations)  for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>X::Array{Float64, 2}</code>: Design matrix of linear regression model.</li><li><code>y::Array{Float64, 1}</code>: Response vector of linear regression model.</li><li><code>exact::Bool</code>: Consider all possible subsets of p or not where p is the number of regression parameters.</li></ul><p><strong>References</strong></p><p>Hawkins, Douglas M., and David Olive. &quot;Applications and algorithms for least trimmed sum of  absolute deviations regression.&quot; Computational Statistics &amp; Data Analysis 32.2 (1999): 119-134.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/lta.jl#L60-L76">source</a></section></article><h2 id="Hadi-(1992)"><a class="docs-heading-anchor" href="#Hadi-(1992)">Hadi (1992)</a><a id="Hadi-(1992)-1"></a><a class="docs-heading-anchor-permalink" href="#Hadi-(1992)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.Hadi92.hadi1992" href="#LinRegOutliers.Hadi92.hadi1992"><code>LinRegOutliers.Hadi92.hadi1992</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">hadi1992(multivariateData)</code></pre><p>Perform Hadi (1992) algorithm for a given multivariate data. </p><p><strong>Arguments</strong></p><ul><li><code>multivariateData::Array{Float64, 2}</code>: Multivariate data.</li></ul><p><strong>Description</strong></p><p>Algorithm starts with an initial subset and enlarges the subset to  obtain robust covariance matrix and location estimates. </p><p><strong>Output</strong></p><ul><li><code>[&quot;outliers&quot;]</code>: Array of indices of outliers</li><li><code>[&quot;critical.chi.squared&quot;]</code>: Threshold value for determining being an outlier</li><li><code>[&quot;rth.robust.distance&quot;]</code>: rth robust distance, where (r+1)th robust distance is the first one that exceeds the threshold.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; multidata = hcat(hbk.x1, hbk.x2, hbk.x3);

julia&gt; hadi1992(multidata)
Dict{Any,Any} with 3 entries:
  &quot;outliers&quot;              =&gt; [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
  &quot;critical.chi.squared&quot; =&gt; 7.81473
  &quot;rth.robust.distance&quot;   =&gt; 5.04541</code></pre><p># Reference Hadi, Ali S. &quot;Identifying multiple outliers in multivariate data.&quot;  Journal of the Royal Statistical Society: Series B (Methodological) 54.3 (1992): 761-771.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/hadi1992.jl#L39-L71">source</a></section></article><h2 id="Marchette-and-Solka-(2003)-Data-Images"><a class="docs-heading-anchor" href="#Marchette-and-Solka-(2003)-Data-Images">Marchette &amp; Solka (2003) Data Images</a><a id="Marchette-and-Solka-(2003)-Data-Images-1"></a><a class="docs-heading-anchor-permalink" href="#Marchette-and-Solka-(2003)-Data-Images" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.DataImage.dataimage" href="#LinRegOutliers.DataImage.dataimage"><code>LinRegOutliers.DataImage.dataimage</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">dataimage(dataMatrix; distance = :mahalanobis)</code></pre><p>Generate the Marchette &amp; Solka (2003) data image for a given data matrix. </p><p><strong>Arguments</strong></p><ul><li><code>dataMatrix::Array{Float64, 1}</code>: Data matrix with dimensions n x p, where n is the number of observations and p is the number of variables.</li><li><code>distance::Symbol</code>: Optional argument for the distance function.</li></ul><p><strong>Notes</strong></p><pre><code class="nohighlight hljs">distance is :mahalanobis by default, for the Mahalanobis distances. 
use 

    dataimage(mat, distance = :euclidean)

to use Euclidean distances.</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; x1 = hbk[:,&quot;x1&quot;];
julia&gt; x2 = hbk[:,&quot;x2&quot;];
julia&gt; x3 = hbk[:,&quot;x3&quot;];
julia&gt; mat = hcat(x1, x2, x3);
julia&gt; dataimage(mat)</code></pre><p><strong>References</strong></p><p>Marchette, David J., and Jeffrey L. Solka. &quot;Using data images for outlier detection.&quot;  Computational Statistics &amp; Data Analysis 43.4 (2003): 541-552.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/dataimage.jl#L12-L42">source</a></section></article><h2 id="Satman&#39;s-GA-based-LTS-estimation-(2012)"><a class="docs-heading-anchor" href="#Satman&#39;s-GA-based-LTS-estimation-(2012)">Satman&#39;s GA based LTS estimation (2012)</a><a id="Satman&#39;s-GA-based-LTS-estimation-(2012)-1"></a><a class="docs-heading-anchor-permalink" href="#Satman&#39;s-GA-based-LTS-estimation-(2012)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.GALTS.galts" href="#LinRegOutliers.GALTS.galts"><code>LinRegOutliers.GALTS.galts</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">galts(setting)</code></pre><p>Perform Satman(2012) algorithm for estimating LTS coefficients.</p><p><strong>Arguments</strong></p><ul><li><code>setting</code>: A regression setting object.</li></ul><p><strong>Description</strong></p><p>The algorithm performs a genetic search for estimating LTS coefficients using C-Steps. </p><p><strong>Output</strong></p><ul><li><code>[&quot;betas&quot;]</code>: Robust regression coefficients</li><li><code>[&quot;best.subset&quot;]</code>: Clean subset of h observations, where h is an integer greater than n / 2. The default value of h is <code>Int(floor((n + p + 1.0) / 2.0))</code>.</li><li><code>[&quot;objective&quot;]</code>: Objective value</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; reg = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; galts(reg)
Dict{Any,Any} with 3 entries:
  &quot;betas&quot;       =&gt; [-56.5219, 1.16488]
  &quot;best.subset&quot; =&gt; [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 23, 24]
  &quot;objective&quot;   =&gt; 3.43133</code></pre><p><strong>References</strong></p><p>Satman, M. Hakan. &quot;A genetic algorithm based modification on the lts algorithm for large data sets.&quot;  Communications in Statistics-Simulation and Computation 41.5 (2012): 644-652.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/gwlts.jl#L74-L104">source</a></section></article><h2 id="Fischler-and-Bolles-(1981)-RANSAC-Algorithm"><a class="docs-heading-anchor" href="#Fischler-and-Bolles-(1981)-RANSAC-Algorithm">Fischler &amp; Bolles (1981) RANSAC Algorithm</a><a id="Fischler-and-Bolles-(1981)-RANSAC-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Fischler-and-Bolles-(1981)-RANSAC-Algorithm" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.Ransac.ransac" href="#LinRegOutliers.Ransac.ransac"><code>LinRegOutliers.Ransac.ransac</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">ransac(setting; t, w=0.5, m=0, k=0, d=0, confidence=0.99)</code></pre><p>Run the RANSAC (1981) algorithm for the given regression setting</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and a dataset.</li><li><code>t::Float64</code>: The threshold distance of a sample point to the regression hyperplane to determine if it fits the model well.</li><li><code>w::Float64</code>: The probability of a sample point being inlier, default=0.5.</li><li><code>m::Int</code>: The number of points to sample to estimate the model parameter for each iteration. If set to 0, defaults to picking p points which is the minimum required.</li><li><code>k::Int</code>: The number of iterations to run. If set to 0, is calculated according to the formula given in the paper based on outlier probability and the sample set size.</li><li><code>d::Int</code>: The number of close data points required to accept the model. Defaults to number of data points multiplied by inlier ratio.</li><li><code>confidence::Float64</code>: Required to determine the number of optimum iterations if k is not specified.</li></ul><p><strong>Output</strong></p><ul><li><code>[&quot;outliers&quot;]</code>: Array of indices of outliers.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; df = DataFrame(y=[0,1,2,3,3,4,10], x=[0,1,2,2,3,4,2])
julia&gt; reg = createRegressionSetting(@formula(y ~ x), df)
julia&gt; ransac(reg, t=0.8, w=0.85)
Dict{String,Array{Int64,1}} with 1 entry:
  &quot;outliers&quot; =&gt; [7]</code></pre><p><strong>References</strong></p><p>Martin A. Fischler &amp; Robert C. Bolles (June 1981). &quot;Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography&quot; Comm. ACM. 24 (6): 381–395.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/ransac.jl#L14-L45">source</a></section></article><h2 id="Minimum-Covariance-Determinant-Estimator-(MCD)"><a class="docs-heading-anchor" href="#Minimum-Covariance-Determinant-Estimator-(MCD)">Minimum Covariance Determinant Estimator (MCD)</a><a id="Minimum-Covariance-Determinant-Estimator-(MCD)-1"></a><a class="docs-heading-anchor-permalink" href="#Minimum-Covariance-Determinant-Estimator-(MCD)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.MVE.mcd" href="#LinRegOutliers.MVE.mcd"><code>LinRegOutliers.MVE.mcd</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">mcd(data; alpha = 0.01)</code></pre><p>Performs the Minimum Covariance Determinant algorithm for a robust covariance matrix.</p><p><strong>Arguments</strong></p><ul><li><code>data::DataFrame</code>: Multivariate data.</li><li><code>alpha::Float64</code>: Probability for quantiles of Chi-Squared statistic.</li></ul><p><strong>Description</strong></p><p><code>mcd</code> searches for a robust location vector and a robust scale matrix, e.g covariance matrix. The method also reports a usable diagnostic measure, Mahalanobis distances, which are calculated using  the robust counterparts instead of mean vector and usual covariance matrix. Mahalanobis distances  are directly comparible with quantiles of a ChiSquare Distribution with <code>p</code> degrees of freedom.  </p><p><strong>Output</strong></p><ul><li><code>[&quot;goal&quot;]</code>: Objective value</li><li><code>[&quot;best.subset&quot;]</code>: Indices of best h-subset of observations</li><li><code>[&quot;robust.location&quot;]</code>: Vector of robust location measures</li><li><code>[&quot;robust.covariance&quot;]</code>: Robust covariance matrix</li><li><code>[&quot;squared.mahalanobis&quot;]</code>: Array of Mahalanobis distances calculated using robust location and scale measures.</li><li><code>[&quot;chisq.crit&quot;]</code>: Chisquare quantile used in threshold</li><li><code>[&quot;alpha&quot;]</code>: Probability used in calculating the Chisquare quantile, e.g <code>chisq.crit</code></li><li><code>[&quot;outliers&quot;]</code>: Array of indices of outliers.</li></ul><p><strong>Notes</strong></p><p>Algorithm is implemented using concentration steps as described in the reference paper. However, details about number of iterations are slightly different.</p><p><strong>References</strong></p><p>Rousseeuw, Peter J., and Katrien Van Driessen. &quot;A fast algorithm for the minimum covariance  determinant estimator.&quot; Technometrics 41.3 (1999): 212-223.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/mve.jl#L125-L159">source</a></section></article><h2 id="Imon-(2005)-Algorithm"><a class="docs-heading-anchor" href="#Imon-(2005)-Algorithm">Imon (2005) Algorithm</a><a id="Imon-(2005)-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Imon-(2005)-Algorithm" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.Imon2005.imon2005" href="#LinRegOutliers.Imon2005.imon2005"><code>LinRegOutliers.Imon2005.imon2005</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">imon2005(setting)</code></pre><p>Perform the Imon 2005 algorithm for a given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: A regression setting.</li></ul><p><strong>Description</strong></p><p>The algorithm estimates the GDFFITS diagnostic, which is an extension of well-known regression  diagnostic DFFITS. Unlikely, GDFFITS is used for detecting multiple outliers whereas the original one was used for detecting single outliers. </p><p><strong>Output</strong></p><ul><li><code>[&quot;crit&quot;]</code>: The critical value used</li><li><code>[&quot;gdffits&quot;]</code>: Array of GDFFITS diagnostic calculated for observations</li><li><code>[&quot;outliers&quot;]</code>: Array of indices of outliers.</li></ul><p><strong>Notes</strong></p><p>The implementation uses LTS rather than LMS as suggested in the paper. </p><p><strong>References</strong></p><p>A. H. M. Rahmatullah Imon (2005) Identifying multiple influential observations in linear regression,  Journal of Applied Statistics, 32:9, 929-946, DOI: 10.1080/02664760500163599</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/imon2005.jl#L9-L33">source</a></section></article><h2 id="Barratt-and-Angeris-and-Boyd-(2020)-CCF-algorithm"><a class="docs-heading-anchor" href="#Barratt-and-Angeris-and-Boyd-(2020)-CCF-algorithm">Barratt &amp; Angeris &amp; Boyd (2020) CCF algorithm</a><a id="Barratt-and-Angeris-and-Boyd-(2020)-CCF-algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Barratt-and-Angeris-and-Boyd-(2020)-CCF-algorithm" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.CCF.ccf" href="#LinRegOutliers.CCF.ccf"><code>LinRegOutliers.CCF.ccf</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">ccf(setting; starting_lambdas = nothing)</code></pre><p>Perform signed gradient descent for clipped convex functions for a given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li><li><code>starting_lambdas::Array{Float64,1}</code>: Starting values of weighting parameters used by signed gradient descent.</li><li><code>alpha::Float64</code>: Loss at which a point is labeled as an outlier (points with loss ≥ alpha will be called outliers).</li><li><code>max_iter::Int64</code>: Maximum number of iterations to run signed gradient descent.</li><li><code>beta::Float64</code>: Step size parameter.</li><li><code>tol::Float64</code>: Tolerance below which convergence is declared.</li></ul><p><strong>Output</strong></p><ul><li><code>[&quot;betas&quot;]</code>: Robust regression coefficients</li><li><code>[&quot;&quot;outliers&quot;]</code>: Array of indices of outliers</li><li><code>[&quot;&quot;lambdas&quot;]</code>: Lambda coefficients estimated in each iteration </li><li><code>[&quot;&quot;residuals&quot;]</code>: Regression residuals.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; reg0001 = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; ccf(reg0001)
Dict{Any,Any} with 4 entries:
  &quot;betas&quot;     =&gt; [-63.4816, 1.30406]
  &quot;outliers&quot;  =&gt; [15, 16, 17, 18, 19, 20]
  &quot;lambdas&quot;   =&gt; [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  2.77556e-17, 2.77556e-17, 0…
  &quot;residuals&quot; =&gt; [-2.67878, -1.67473, -0.37067, -0.266613, 0.337444, 0.941501, 1.44556, 2.04962, 1…
</code></pre><p><strong>References</strong></p><p>Barratt, S., Angeris, G. &amp; Boyd, S. Minimizing a sum of clipped convex functions. Optim Lett 14, 2443–2459 (2020). https://doi.org/10.1007/s11590-020-01565-4</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/ccf.jl#L13-L50">source</a></section><section><div><pre><code class="nohighlight hljs">ccf(X, y; starting_lambdas = nothing)</code></pre><p>Perform signed gradient descent for clipped convex functions for a given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>X::Array{Float64, 2}</code>: Design matrix of the linear model.</li><li><code>y::Array{Float64, 1}</code>: Response vector of the linear model.</li><li><code>starting_lambdas::Array{Float64,1}</code>: Starting values of weighting parameters used by signed gradient descent.</li><li><code>alpha::Float64</code>: Loss at which a point is labeled as an outlier. If unspecified, will be chosen as p*mean(residuals.^2), where residuals are OLS residuals.</li><li><code>p::Float64</code>: Points that have squared OLS residual greater than p times the mean squared OLS residual are considered outliers.</li><li><code>max_iter::Int64</code>: Maximum number of iterations to run signed gradient descent.</li><li><code>beta::Float64</code>: Step size parameter.</li><li><code>tol::Float64</code>: Tolerance below which convergence is declared.</li></ul><p><strong>Output</strong></p><ul><li><code>[&quot;betas&quot;]</code>: Robust regression coefficients</li><li><code>[&quot;&quot;outliers&quot;]</code>: Array of indices of outliers</li><li><code>[&quot;&quot;lambdas&quot;]</code>: Lambda coefficients estimated in each iteration </li><li><code>[&quot;&quot;residuals&quot;]</code>: Regression residuals.</li></ul><p><strong>References</strong></p><p>Barratt, S., Angeris, G. &amp; Boyd, S. Minimizing a sum of clipped convex functions. Optim Lett 14, 2443–2459 (2020). https://doi.org/10.1007/s11590-020-01565-4</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/ccf.jl#L57-L83">source</a></section></article><h2 id="Atkinson-(1994)-Forward-Search-Algorithm"><a class="docs-heading-anchor" href="#Atkinson-(1994)-Forward-Search-Algorithm">Atkinson (1994) Forward Search Algorithm</a><a id="Atkinson-(1994)-Forward-Search-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Atkinson-(1994)-Forward-Search-Algorithm" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.Atkinson94.atkinson94" href="#LinRegOutliers.Atkinson94.atkinson94"><code>LinRegOutliers.Atkinson94.atkinson94</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">    atkinson94(setting, iters, crit)</code></pre><p>Runs the Atkinson94 algorithm to find out outliers using LMS method.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: A regression setting object.</li><li><code>iters::Int</code>: Number of random samples.</li><li><code>crit::Float64</code>: Critical value for residuals</li></ul><p><strong>Description</strong></p><p>The algorithm randomly selects initial basic subsets and performs a very robust method, e.g <code>lms</code> to enlarge the basic subset. In each iteration of forward search, the best objective value and parameter  estimates are stored. These values are also used in Atkinson&#39;s Stalactite Plot for a visual investigation of  outliers. See <code>atkinsonstalactiteplot</code>.</p><p><strong>Output</strong></p><ul><li><code>[&quot;optimum_index&quot;]</code>: The iteration number in which the minimum objective is obtained</li><li><code>[&quot;residuals_matrix&quot;]</code>: Matrix of residuals obtained in each iteration</li><li><code>[&quot;outliers&quot;]</code>: Array of indices of detected outliers</li><li><code>[&quot;objective&quot;]</code>: Minimum objective value</li><li><code>[&quot;coef&quot;]</code>: Estimated regression coefficients</li><li><code>[&quot;crit&quot;]</code>: Critical value given by the user.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; reg = createRegressionSetting(@formula(stackloss ~ airflow + watertemp + acidcond), stackloss)
julia&gt; atkinson94(reg)
Dict{Any,Any} with 6 entries:
  &quot;optimum_index&quot;    =&gt; 10
  &quot;residuals_matrix&quot; =&gt; [0.0286208 0.0620609 … 0.0796249 0.0; 0.0397778 0.120547 … 0.118437 0.0397778; … ; 1.21133 1.80846 … 0.690327 4.14366; 1.61977 0.971592 … 0.616204 3.58098]
  &quot;outliers&quot;         =&gt; [1, 3, 4, 21]
  &quot;objective&quot;        =&gt; 0.799134
  &quot;coef&quot;             =&gt; [-38.3133, 0.745659, 0.432794, 0.0104587]
  &quot;crit&quot;             =&gt; 3.0
</code></pre><p><strong>References</strong></p><p>Atkinson, Anthony C. &quot;Fast very robust methods for the detection of multiple outliers.&quot; Journal of the American Statistical Association 89.428 (1994): 1329-1339.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/atkinson94.jl#L13-L54">source</a></section></article><h2 id="BACON-Algorithm-(Billor-and-Hadi-and-Velleman-(2000))"><a class="docs-heading-anchor" href="#BACON-Algorithm-(Billor-and-Hadi-and-Velleman-(2000))">BACON Algorithm (Billor &amp; Hadi &amp; Velleman (2000))</a><a id="BACON-Algorithm-(Billor-and-Hadi-and-Velleman-(2000))-1"></a><a class="docs-heading-anchor-permalink" href="#BACON-Algorithm-(Billor-and-Hadi-and-Velleman-(2000))" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.Bacon.bacon" href="#LinRegOutliers.Bacon.bacon"><code>LinRegOutliers.Bacon.bacon</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">    bacon(setting, m, method, alpha)</code></pre><p>Run the BACON algorithm to detect outliers on regression data.</p><p><strong>Arguments:</strong></p><ul><li><code>setting</code>: RegressionSetting object with a formula and a dataset.</li><li><code>m</code>: The number of elements to be included in the initial subset.</li><li><code>method</code>: The distance method to use for selecting the points for initial subset</li><li><code>alpha</code>: The quantile used for cutoff</li></ul><p><strong>Description</strong></p><p>BACON (Blocked Adaptive Computationally efficient Outlier Nominators) algoritm, defined in the citation below, has many versions, e.g BACON for multivariate data, BACON for regression etc. Since the design matrix of a regression model is multivariate data, BACON for multivariate data is performed in early stages of the algorithm. After selecting a clean subset of observations, then a forward search is applied. Observations with high studendized residuals are reported as outliers.</p><p><strong>Output</strong></p><ul><li><code>[&quot;outliers&quot;]</code>: Array of indices of outliers.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; reg = createRegressionSetting(@formula(stackloss ~ airflow + watertemp + acidcond), stackloss)
julia&gt; bacon(reg, m=12)
Dict{String,Array{Int64,1}} with 1 entry:
  &quot;outliers&quot; =&gt; [1, 3, 4, 21]</code></pre><p><strong>References</strong></p><p>Billor, Nedret, Ali S. Hadi, and Paul F. Velleman. &quot;BACON: blocked adaptive computationally efficient outlier nominators.&quot; Computational statistics &amp; data analysis 34.3 (2000): 279-298.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/bacon.jl#L180-L210">source</a></section></article><h2 id="Hadi-(1994)-Algorithm"><a class="docs-heading-anchor" href="#Hadi-(1994)-Algorithm">Hadi (1994) Algorithm</a><a id="Hadi-(1994)-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Hadi-(1994)-Algorithm" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.Hadi94.hadi1994" href="#LinRegOutliers.Hadi94.hadi1994"><code>LinRegOutliers.Hadi94.hadi1994</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">hadi1994(multivariateData)</code></pre><p>Perform Hadi (1994) algorithm for a given multivariate data.</p><p><strong>Arguments</strong></p><ul><li><code>multivariateData::Array{Float64, 2}</code>: Multivariate data.</li></ul><p><strong>Description</strong></p><p>Algorithm starts with an initial subset and enlarges the subset to  obtain robust covariance matrix and location estimates. This algorithm  is an extension of <code>hadi1992</code>.</p><p><strong>Output</strong></p><ul><li><code>[&quot;outliers&quot;]</code>: Array of indices of outliers</li><li><code>[&quot;critical.chi.squared&quot;]</code>: Threshold value for determining being an outlier</li><li><code>[&quot;rth.robust.distance&quot;]</code>: rth robust distance, where (r+1)th robust distance is the first one that exceeds the threshold.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; multidata = hcat(hbk.x1, hbk.x2, hbk.x3);

julia&gt; hadi1994(multidata)
Dict{Any,Any} with 3 entries:
  &quot;outliers&quot;              =&gt; [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
  &quot;critical.chi.squared&quot; =&gt; 7.81473
  &quot;rth.robust.distance&quot;   =&gt; 5.04541</code></pre><p># Reference Hadi, Ali S. &quot;A modification of a method for the dedection of outliers in multivariate samples&quot; Journal of the Royal Statistical Society: Series B (Methodological) 56.2 (1994): 393-396.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/hadi1994.jl#L16-L49">source</a></section></article><h2 id="Chatterjee-and-Mächler-(1997)"><a class="docs-heading-anchor" href="#Chatterjee-and-Mächler-(1997)">Chatterjee &amp; Mächler (1997)</a><a id="Chatterjee-and-Mächler-(1997)-1"></a><a class="docs-heading-anchor-permalink" href="#Chatterjee-and-Mächler-(1997)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.CM97.cm97" href="#LinRegOutliers.CM97.cm97"><code>LinRegOutliers.CM97.cm97</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">cm97(setting; maxiter = 1000)</code></pre><p>Perform the Chatterjee and Mächler (1997) algorithm for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li></ul><p><strong>Description</strong></p><p>The algorithm performs a iteratively weighted least squares estimation to obtain robust regression coefficients.</p><p><strong>Output</strong></p><ul><li><code>[&quot;betas&quot;]</code>: Robust regression coefficients</li><li><code>[&quot;iterations&quot;]</code>: Number of iterations performed</li><li><code>[&quot;converged&quot;]</code>: true if the algorithm converges, otherwise, false.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; myreg = createRegressionSetting(@formula(stackloss ~ airflow + watertemp + acidcond), stackloss)
julia&gt; result = cm97(myreg)
Dict{String,Any} with 3 entries:
  &quot;betas&quot;      =&gt; [-37.0007, 0.839285, 0.632333, -0.113208]
  &quot;iterations&quot; =&gt; 22
  &quot;converged&quot;  =&gt; true</code></pre><p><strong>References</strong></p><p>Chatterjee, Samprit, and Martin Mächler. &quot;Robust regression: A weighted least squares approach.&quot;  Communications in Statistics-Theory and Methods 26.6 (1997): 1381-1394.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jbytecode/LinRegOutliers/blob/0f03986b7d949c2160ad1821177105d91f8f7dff/src/cm97.jl#L11-L42">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="../diagnostics/">Diagnostics »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.22 on <span class="colophon-date" title="Thursday 25 August 2022 09:44">Thursday 25 August 2022</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
