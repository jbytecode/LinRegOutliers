var documenterSearchIndex = {"docs":
[{"location":"datasets/#Datasets","page":"Datasets","title":"Datasets","text":"","category":"section"},{"location":"datasets/#Phone-data","page":"Datasets","title":"Phone data","text":"","category":"section"},{"location":"datasets/","page":"Datasets","title":"Datasets","text":"LinRegOutliers.phones","category":"page"},{"location":"datasets/#LinRegOutliers.DataSets.phones","page":"Datasets","title":"LinRegOutliers.DataSets.phones","text":"Phone data\n\nComponents\n\nyear::Integer: years from 1950 to 1973.\ncalls::Float64: phone calls (in millions).\n\nReference\n\nP. J. Rousseeuw and A. M. Leroy (1987) Robust Regression &      Outlier Detection. Wiley.\n\n\n\n\n\n","category":"constant"},{"location":"datasets/#Hawkings-and-Bradu-and-Kass-data","page":"Datasets","title":"Hawkings & Bradu & Kass data","text":"","category":"section"},{"location":"datasets/","page":"Datasets","title":"Datasets","text":"LinRegOutliers.hbk","category":"page"},{"location":"datasets/#LinRegOutliers.DataSets.hbk","page":"Datasets","title":"LinRegOutliers.DataSets.hbk","text":"Hawkins & Bradu & Kass data\n\nComponents\n\nx1::Float64: first independent variable.\nx2::Float64: second independent variable.\nx3::Float64: third independent variable.\ny::Float64: dependent (response) variable.\n\nReference\n\nHawkins, D.M., Bradu, D., and Kass, G.V. (1984) Location of several outliers in multiple regression data using elemental sets. Technometrics 26, 197–208.\n\n\n\n\n\n","category":"constant"},{"location":"datasets/#Animals-data","page":"Datasets","title":"Animals data","text":"","category":"section"},{"location":"datasets/","page":"Datasets","title":"Datasets","text":"LinRegOutliers.animals","category":"page"},{"location":"datasets/#LinRegOutliers.DataSets.animals","page":"Datasets","title":"LinRegOutliers.DataSets.animals","text":"Animals data\n\nComponents\n\nnames::AbstractString: names of animals.\nbody::Float64: body weight in kg.\nbrain::Float64: brain weight in g.\n\nReferences\n\n Venables, W. N. and Ripley, B. D. (1999) _Modern Applied\n Statistics with S-PLUS._ Third Edition. Springer.\n\n P. J. Rousseeuw and A. M. Leroy (1987) _Robust Regression and\n Outlier Detection._ Wiley, p. 57.\n\n\n\n\n\n","category":"constant"},{"location":"datasets/#Weight-Loss-data","page":"Datasets","title":"Weight Loss data","text":"","category":"section"},{"location":"datasets/","page":"Datasets","title":"Datasets","text":"LinRegOutliers.weightloss","category":"page"},{"location":"datasets/#LinRegOutliers.DataSets.weightloss","page":"Datasets","title":"LinRegOutliers.DataSets.weightloss","text":"Weight loss data\n\nComponents\n\ndays::Integer: time in days since the start of the diet program.\nweight::Float64: weight in kg.\n\nReference\n\n Venables, W. N. and Ripley, B. D. (1999) _Modern Applied\n Statistics with S-PLUS._ Third Edition. Springer.\n\n\n\n\n\n","category":"constant"},{"location":"datasets/#Stack-Loss-data","page":"Datasets","title":"Stack Loss data","text":"","category":"section"},{"location":"datasets/","page":"Datasets","title":"Datasets","text":"LinRegOutliers.stackloss","category":"page"},{"location":"datasets/#LinRegOutliers.DataSets.stackloss","page":"Datasets","title":"LinRegOutliers.DataSets.stackloss","text":"Stack loss data\n\nComponents\n\nairflow::Float64: flow of cooling air (independent variable).\nwatertemp::Float64: cooling water inlet temperature (independent variable).\nacidcond::Float64: concentration of acid (independent variable).\nstackloss::Float64: stack loss (dependent variable).\n\nOutliers\n\nObservations 1, 3, 4, and 21 are outliers.\n\nReferences\n\nBecker, R. A., Chambers, J. M. and Wilks, A. R. (1988) _The New S Language_.  Wadsworth & Brooks/Cole.\n\nDodge, Y. (1996) The guinea pig of multiple regression. In: _Robust Statistics, Data Analysis, and Computer Intensive Methods;\nIn Honor of Peter Huber's 60th Birthday_, 1996, _Lecture Notes in Statistics_ *109*, Springer-Verlag, New York.\n\n\n\n\n\n","category":"constant"},{"location":"datasets/#Hadi-and-Simonoff-(1993)-random-data","page":"Datasets","title":"Hadi & Simonoff (1993) random data","text":"","category":"section"},{"location":"datasets/","page":"Datasets","title":"Datasets","text":"LinRegOutliers.hs93randomdata","category":"page"},{"location":"datasets/#LinRegOutliers.DataSets.hs93randomdata","page":"Datasets","title":"LinRegOutliers.DataSets.hs93randomdata","text":"Hadi & Simonoff (1993) Random data\n\nComponents\n\nx1::Float64: Random values.\nx2::Float64: Random values.\ny::Float64: Random values (independent variable).\n\nOutliers\n\nObservations 1, 2, and 3 are outliers.\n\nReferences\n\nHadi, Ali S., and Jeffrey S. Simonoff. \"Procedures for the identification of  multiple outliers in linear models.\" Journal of the American Statistical  Association 88.424 (1993): 1264-1272.\n\n\n\n\n\n","category":"constant"},{"location":"datasets/#Modified-Wood-Gravity-data","page":"Datasets","title":"Modified Wood Gravity data","text":"","category":"section"},{"location":"datasets/","page":"Datasets","title":"Datasets","text":"LinRegOutliers.woodgravity","category":"page"},{"location":"datasets/#LinRegOutliers.DataSets.woodgravity","page":"Datasets","title":"LinRegOutliers.DataSets.woodgravity","text":"Modified Wood Gravity Data\n\nComponents\n\nx1::Float64: Random values.\nx2::Float64: Random values.\nx3::Float64: Random values.\nx4::Float64: Random values.\nx5::Float64: Random values.\ny::Float64: Random values (independent variable).\n\nReferences\n\nP. J. Rousseeuw and A. M. Leroy (1987) Robust Regression and Outlier Detection. Wiley, p.243, table 8.\n\n\n\n\n\n","category":"constant"},{"location":"datasets/#Scottish-Hill-Races-data","page":"Datasets","title":"Scottish Hill Races data","text":"","category":"section"},{"location":"datasets/","page":"Datasets","title":"Datasets","text":"LinRegOutliers.hills","category":"page"},{"location":"datasets/#LinRegOutliers.DataSets.hills","page":"Datasets","title":"LinRegOutliers.DataSets.hills","text":"Scottish Hill Races Data\n\nComponents\n\ndist::AbstractVector{Float64}: Distance in miles (Independent). \nclimb::AbstractVector{Float64}: Heights in feet (Independent).\ntime::AbstractVector{Float64}: Record times in hours (Dependent).\n\nModel\n\ntime ~ dist + climb\n\nReferences\n\nA.C. Atkinson (1986) Comment: Aspects of diagnostic regression analysis. Statistical Science 1, 397-402.\n\n\n\n\n\n","category":"constant"},{"location":"datasets/#Soft-Drink-Delivery-data","page":"Datasets","title":"Soft Drink Delivery data","text":"","category":"section"},{"location":"datasets/","page":"Datasets","title":"Datasets","text":"LinRegOutliers.softdrinkdelivery","category":"page"},{"location":"datasets/#LinRegOutliers.DataSets.softdrinkdelivery","page":"Datasets","title":"LinRegOutliers.DataSets.softdrinkdelivery","text":"Soft Drink Delivery Data\n\nComponents\n\ncases::AbstractVector{Float64}: Independent variable. \ndistance::AbstractVector{Float64}: Independent variable. \ntime::AbstractVector{Float64}: Dependent variable. \n\nModel\n\ntime ~ distance + cases \n\nReference\n\nD. C. Montgomery and E. A. Peck (1992) Introduction to Regression Analysis. Wiley, New York. \n\n\n\n\n\n","category":"constant"},{"location":"types/#Types","page":"Types","title":"Types","text":"","category":"section"},{"location":"types/#RegressionSetting","page":"Types","title":"RegressionSetting","text":"","category":"section"},{"location":"types/","page":"Types","title":"Types","text":"LinRegOutliers.RegressionSetting","category":"page"},{"location":"types/#LinRegOutliers.Basis.RegressionSetting","page":"Types","title":"LinRegOutliers.Basis.RegressionSetting","text":"struct RegressionSetting\n    formula::FormulaTerm\n    data::DataFrame\nend\n\nImmutable data structure for a regression setting.\n\nArguments\n\nformula::FormulaTerm: A formula object describes the linear regression model.\ndata::DataFrame: DataFrame object holds the data\n\nNotes\n\nImplemented methods in this packages accepts linear models as RegressionSetting objects.\nThis objects holds the model formula and the data used in regression estimations.\n\nExamples\n\njulia> setting = RegressionSetting(@formula(calls ~ year), phones)\nRegressionSetting(calls ~ year, 24×2 DataFrame\n│ Row │ year  │ calls   │\n│     │ Int64 │ Float64 │\n├─────┼───────┼─────────┤\n│ 1   │ 50    │ 4.4     │\n│ 2   │ 51    │ 4.7     │\n│ 3   │ 52    │ 4.7     │\n│ 4   │ 53    │ 5.9     │\n│ 5   │ 54    │ 6.6     │\n│ 6   │ 55    │ 7.3     │\n│ 7   │ 56    │ 8.1     │\n│ 8   │ 57    │ 8.8     │\n│ 9   │ 58    │ 10.6    │\n│ 10  │ 59    │ 12.0    │\n⋮\n│ 14  │ 63    │ 21.2    │\n│ 15  │ 64    │ 119.0   │\n│ 16  │ 65    │ 124.0   │\n│ 17  │ 66    │ 142.0   │\n│ 18  │ 67    │ 159.0   │\n│ 19  │ 68    │ 182.0   │\n│ 20  │ 69    │ 212.0   │\n│ 21  │ 70    │ 43.0    │\n│ 22  │ 71    │ 24.0    │\n│ 23  │ 72    │ 27.0    │\n│ 24  │ 73    │ 29.0    │)\n\n\n\n\n\n","category":"type"},{"location":"diagnostics/#Diagnostics","page":"Diagnostics","title":"Diagnostics","text":"","category":"section"},{"location":"diagnostics/#wls","page":"Diagnostics","title":"wls","text":"","category":"section"},{"location":"diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"LinRegOutliers.wls","category":"page"},{"location":"diagnostics/#LinRegOutliers.OrdinaryLeastSquares.wls","page":"Diagnostics","title":"LinRegOutliers.OrdinaryLeastSquares.wls","text":"wls(X, y, wts)\n\nEstimate weighted least squares regression and create OLS object with estimated parameters.\n\nArguments\n\nX::AbstractMatrix{Float64}: Design matrix.\ny::AbstractVector{Float64}: Response vector.\nwts::AbstractVector{Float64}: Weights vector.\n\nExamples\n\njulia> X = hcat(ones(24), phones[:,\"year\"]);\njulia> y = phones[:,\"calls\"];\njulia> w = ones(24)\njulia> w[15:20] .= 0.0\njulia> reg = wls(X, y, w)\njulia> reg.betas\n2-element Vector{Float64}:\n -63.481644325290425\n   1.3040571939231453\n\n\n\n\n\n","category":"function"},{"location":"diagnostics/#dffit","page":"Diagnostics","title":"dffit","text":"","category":"section"},{"location":"diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"LinRegOutliers.dffit","category":"page"},{"location":"diagnostics/#LinRegOutliers.Diagnostics.dffit","page":"Diagnostics","title":"LinRegOutliers.Diagnostics.dffit","text":"dffit(setting, i)\n\nCalculate the effect of the ith observation on the linear regression fit.\n\nArguments\n\nsetting::RegressionSetting: A regression setting object.\ni::Int: Index of the observation.\n\nExamples\n\njulia> reg = createRegressionSetting(@formula(calls ~ year), phones);\njulia> dffit(reg, 1)\n2.3008326745719785\n\njulia> dffit(reg, 15)\n2.7880619386124295\n\njulia> dffit(reg, 16)\n3.1116532421969794\n\njulia> dffit(reg, 17)\n4.367981450347031\n\njulia> dffit(reg, 21)\n-5.81610150322166\n\nReferences\n\nBelsley, David A., Edwin Kuh, and Roy E. Welsch. Regression diagnostics:  Identifying influential data and sources of collinearity. Vol. 571. John Wiley & Sons, 2005.\n\n\n\n\n\n","category":"function"},{"location":"diagnostics/#dffits","page":"Diagnostics","title":"dffits","text":"","category":"section"},{"location":"diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"LinRegOutliers.dffits","category":"page"},{"location":"diagnostics/#LinRegOutliers.Diagnostics.dffits","page":"Diagnostics","title":"LinRegOutliers.Diagnostics.dffits","text":"dffits(setting)\n\nCalculate dffit for all observations.\n\nArguments\n\nsetting::RegressionSetting: A regression setting object.\n\nExamples\n\njulia> reg = createRegressionSetting(@formula(calls ~ year), phones);\n\njulia> dffits(reg)\n24-element Vector{Float64}:\n   2.3008326745719785\n   1.2189579001467337\n   0.35535667547543426\n  -0.14458523141740898\n  -0.5558346324846752\n  -0.8441316814464983\n  -1.0329184407957257\n  -1.16600692151232\n  -1.2005633711667656\n  -1.2549187193476428\n  -1.3195581500053777\n  -1.42383876236147\n  -1.5917690629803474\n  -1.6582086833534504\n   2.7880619386124295\n   3.1116532421969794\n   4.367981450347031\n   5.927603041427858\n   8.442860517217582\n  12.370243663029527\n  -5.81610150322166\n -10.089153963127842\n -12.10803256546825\n -14.67006851119936\n\nReferences\n\nBelsley, David A., Edwin Kuh, and Roy E. Welsch. Regression diagnostics:  Identifying influential data and sources of collinearity. Vol. 571. John Wiley & Sons, 2005.\n\n\n\n\n\n","category":"function"},{"location":"diagnostics/#hatmatrix","page":"Diagnostics","title":"hatmatrix","text":"","category":"section"},{"location":"diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"LinRegOutliers.hatmatrix","category":"page"},{"location":"diagnostics/#LinRegOutliers.Diagnostics.hatmatrix","page":"Diagnostics","title":"LinRegOutliers.Diagnostics.hatmatrix","text":"hatmatrix(setting)\n\nCalculate Hat matrix of dimensions n x n for a given regression setting with n observations.\n\nArguments\n\nsetting::RegressionSetting: A regression setting object.\n\nExamples\n\njulia> reg = createRegressionSetting(@formula(calls ~ year), phones);\njulia> size(hatmatrix(reg))\n\n(24, 24)\n\n\n\n\n\n","category":"function"},{"location":"diagnostics/#studentizedResiduals","page":"Diagnostics","title":"studentizedResiduals","text":"","category":"section"},{"location":"diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"LinRegOutliers.studentizedResiduals","category":"page"},{"location":"diagnostics/#LinRegOutliers.Diagnostics.studentizedResiduals","page":"Diagnostics","title":"LinRegOutliers.Diagnostics.studentizedResiduals","text":"studentizedResiduals(setting)\n\nCalculate Studentized residuals for a given regression setting.\n\n# Arguments:\n\nsetting::RegressionSetting: A regression setting object.\n\nExamples\n\njulia> reg = createRegressionSetting(@formula(calls ~ year), phones);\n\njulia> studentizedResiduals(reg)\n24-element Vector{Float64}:\n  0.2398783264505892\n  0.1463945666608097\n  0.04934549995087145\n -0.023289236798461784\n -0.10408303320973748\n -0.18382934382804111\n -0.2609395640240455\n -0.33934473417314376\n -0.3973205657179429\n -0.46258080183149236\n -0.5261488085924144\n -0.5918396227060093\n -0.6616423337899147\n -0.6611792918262785\n  1.0277190922689816\n  1.0297863954540103\n  1.2712201589839855\n  1.4974523565936426\n  1.8386296155264197\n  2.316394853333409\n -0.9368354141338643\n -1.4009989983319822\n -1.4541520919831887\n -1.529459974327181\n\n\n\n\n\n","category":"function"},{"location":"diagnostics/#adjustedResiduals","page":"Diagnostics","title":"adjustedResiduals","text":"","category":"section"},{"location":"diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"LinRegOutliers.adjustedResiduals","category":"page"},{"location":"diagnostics/#LinRegOutliers.Diagnostics.adjustedResiduals","page":"Diagnostics","title":"LinRegOutliers.Diagnostics.adjustedResiduals","text":"adjustedResiduals(setting)\n\nCalculate adjusted residuals for a given regression setting.\n\n# Arguments:\n\nsetting::RegressionSetting: A regression setting object.\n\nExamples\n\njulia> reg = createRegressionSetting(@formula(calls ~ year), phones);\njulia> adjustedResiduals(reg)\n24-element Vector{Float64}:\n  13.486773572526268\n   8.2307993473897\n   2.774371467851612\n  -1.3093999279776498\n  -5.851901346871404\n -10.335509559699863\n -14.670907823058053\n -19.07911256736661\n -22.338710565623828\n -26.00786250934617\n -29.58187157605512\n -33.27523207616458\n -37.19977737822219\n -37.173743587631165\n  57.781855070799956\n  57.898085871534626\n  71.47231139524963\n  84.19185329435882\n 103.37399662263209\n 130.23557965295348\n -52.6720662600165\n -78.76891816539992\n -81.75736547266746\n -85.9914301855088\n\n\n\n\n\n","category":"function"},{"location":"diagnostics/#jacknifedS","page":"Diagnostics","title":"jacknifedS","text":"","category":"section"},{"location":"diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"LinRegOutliers.jacknifedS","category":"page"},{"location":"diagnostics/#LinRegOutliers.Diagnostics.jacknifedS","page":"Diagnostics","title":"LinRegOutliers.Diagnostics.jacknifedS","text":"jacknifedS(setting, k)\n\nEstimate standard error of regression with the kth observation is dropped.\n\n# Arguments\n\nsetting::RegressionSetting: A regression setting object.\nk::Int: Index of the omitted observation. \n\n# Examples\n\njulia> reg = createRegressionSetting(@formula(calls ~ year), phones);\njulia> jacknifedS(reg, 2)\n57.518441664761035\n\njulia> jacknifedS(reg, 15)\n56.14810222161477\n\n\n\n\n\n","category":"function"},{"location":"diagnostics/#cooks","page":"Diagnostics","title":"cooks","text":"","category":"section"},{"location":"diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"LinRegOutliers.cooks","category":"page"},{"location":"diagnostics/#LinRegOutliers.Diagnostics.cooks","page":"Diagnostics","title":"LinRegOutliers.Diagnostics.cooks","text":"cooks(setting)\n\nCalculate Cook distances for all observations in a regression setting.\n\nArguments\n\nsetting::RegressionSetting: A regression setting object.\n\nExamples\n\njulia> reg = createRegressionSetting(@formula(calls ~ year), phones);\n\njulia> cooks(reg)\n24-element Vector{Float64}:\n 0.005344774190779822\n 0.0017088194691033689\n 0.00016624914057962608\n 3.1644452583114795e-5\n 0.0005395058666404081\n 0.0014375008774859539\n 0.0024828140956511258\n 0.0036279720445167277\n 0.004357605989540906\n 0.005288503758364767\n 0.006313578057565415\n 0.0076561205696857254\n 0.009568574875389256\n 0.009970039008782357\n 0.02610396373381051\n 0.029272523880917646\n 0.05091236198400663\n 0.08176555044049343\n 0.14380266904640235\n 0.26721539425047447\n 0.051205153558783356\n 0.13401084683481085\n 0.16860324592350226\n 0.2172819114905912\n\nReferences\n\nCook, R. Dennis. \"Detection of influential observation in linear regression.\"  Technometrics 19.1 (1977): 15-18.\n\n\n\n\n\n","category":"function"},{"location":"diagnostics/#cooksoutliers","page":"Diagnostics","title":"cooksoutliers","text":"","category":"section"},{"location":"diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"LinRegOutliers.cooksoutliers ","category":"page"},{"location":"diagnostics/#LinRegOutliers.Diagnostics.cooksoutliers","page":"Diagnostics","title":"LinRegOutliers.Diagnostics.cooksoutliers","text":"cooksoutliers(setting; alpha = 0.5)\n\nCalculates Cooks distance for a given regression setting and reports the potentials outliers\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\nalpha::Float: Probability for cutoff value. quantile(Fdist(p, n-p), alpha) is used for cutoff. Default is 0.5.\n\nOutput\n\n[\"distance\"]: Cooks distances.\n[\"cutoff\"]: Quantile of the F distribution.\n[\"potentials\"]: Vector of indices of potential regression outliers.\n\n\n\n\n\n","category":"function"},{"location":"diagnostics/#mahalanobisSquaredMatrix","page":"Diagnostics","title":"mahalanobisSquaredMatrix","text":"","category":"section"},{"location":"diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"LinRegOutliers.mahalanobisSquaredMatrix","category":"page"},{"location":"diagnostics/#LinRegOutliers.Diagnostics.mahalanobisSquaredMatrix","page":"Diagnostics","title":"LinRegOutliers.Diagnostics.mahalanobisSquaredMatrix","text":"mahalanobisSquaredMatrix(data::DataFrame; meanvector=nothing, covmatrix=nothing)\n\nCalculate Mahalanobis distances.\n\nArguments\n\ndata::DataFrame: A DataFrame object of the multivariate data.\nmeanvector::AbstractVector{Float64}: Optional mean vector of variables.\ncovmatrix::AbstractMatrix{Float64}: Optional covariance matrix of data.\n\n# References\n\nMahalanobis, Prasanta Chandra. \"On the generalized distance in statistics.\"  National Institute of Science of India, 1936.\n\n\n\n\n\n","category":"function"},{"location":"diagnostics/#dfbeta","page":"Diagnostics","title":"dfbeta","text":"","category":"section"},{"location":"diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"LinRegOutliers.dfbeta","category":"page"},{"location":"diagnostics/#LinRegOutliers.Diagnostics.dfbeta","page":"Diagnostics","title":"LinRegOutliers.Diagnostics.dfbeta","text":"dfbeta(setting, omittedIndex)\n\nApply DFBETA diagnostic for a given regression setting and observation index.\n\nArguments\n\nsetting::RegressionSetting: A regression setting object.\nomittedIndex::Int: Index of the omitted observation.\n\nExample\n\njulia> setting = createRegressionSetting(@formula(calls ~ year), phones);\njulia> dfbeta(setting, 1)\n2-element Vector{Float64}:\n  9.643915678524024\n -0.14686166007904422\n\n\n\n\n\n","category":"function"},{"location":"diagnostics/#dfbetas","page":"Diagnostics","title":"dfbetas","text":"","category":"section"},{"location":"diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"LinRegOutliers.dfbetas","category":"page"},{"location":"diagnostics/#LinRegOutliers.Diagnostics.dfbetas","page":"Diagnostics","title":"LinRegOutliers.Diagnostics.dfbetas","text":"dfbetas(setting)\n\nApply DFBETA diagnostic of all of the observations for a given regression setting.\n\nArguments\n\nsetting::RegressionSetting: A regression setting object.\n\nSee also: dfbeta\n\n\n\n\n\n","category":"function"},{"location":"diagnostics/#covratio","page":"Diagnostics","title":"covratio","text":"","category":"section"},{"location":"diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"LinRegOutliers.covratio","category":"page"},{"location":"diagnostics/#LinRegOutliers.Diagnostics.covratio","page":"Diagnostics","title":"LinRegOutliers.Diagnostics.covratio","text":"covratio(setting, omittedIndex)\n\nApply covariance ratio diagnostic for a given regression setting and observation index.\n\nArguments\n\nsetting::RegressionSetting: A regression setting object.\nomittedIndex::Int: Index of the omitted observation.\n\nExample\n\njulia> setting = createRegressionSetting(@formula(calls ~ year), phones);\njulia> covratio(setting, 1)\n1.2945913799871505\n\n\n\n\n\n","category":"function"},{"location":"diagnostics/#hadimeasure","page":"Diagnostics","title":"hadimeasure","text":"","category":"section"},{"location":"diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"LinRegOutliers.hadimeasure","category":"page"},{"location":"diagnostics/#LinRegOutliers.Diagnostics.hadimeasure","page":"Diagnostics","title":"LinRegOutliers.Diagnostics.hadimeasure","text":"hadimeasure(setting; c = 2.0)\n\nApply Hadi's regression diagnostic for a given regression setting\n\nArguments\n\nsetting::RegressionSetting: A regression setting object.\nc::Float64: Critical value selected between 2.0 - 3.0. The default is 2.0.\n\nExample\n\njulia> setting = createRegressionSetting(@formula(calls ~ year), phones);\njulia> hadimeasure(setting)\n\nReferences\n\nChatterjee, Samprit and Hadi, Ali. Regression Analysis by Example. \t 5th ed. N.p.: John Wiley & Sons, 2012.\n\n\n\n\n\n","category":"function"},{"location":"diagnostics/#diagnose","page":"Diagnostics","title":"diagnose","text":"","category":"section"},{"location":"diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"LinRegOutliers.diagnose","category":"page"},{"location":"diagnostics/#LinRegOutliers.Diagnostics.diagnose","page":"Diagnostics","title":"LinRegOutliers.Diagnostics.diagnose","text":"diagnose(setting; alpha = 0.5)\n\nDiagnose a regression setting and report potential outliers using dffits, dfbetas cooks, and hatmatrix\n\nArguments\n\nsetting::RegressionSetting: A regression setting object.\nalpha: Alpha value for Cooks distance cutoff. See cooksoutliers.\n\n\n\n\n\n","category":"function"},{"location":"#LinRegOutliers","page":"LinRegOutliers","title":"LinRegOutliers","text":"","category":"section"},{"location":"","page":"LinRegOutliers","title":"LinRegOutliers","text":"A Julia package for outlier detection in linear regression.","category":"page"},{"location":"#Implemented-Methods","page":"LinRegOutliers","title":"Implemented Methods","text":"","category":"section"},{"location":"","page":"LinRegOutliers","title":"LinRegOutliers","text":"Ordinary Least Squares and Weighted Least Squares regression \nRegression diagnostics (DFBETA, DFFIT, CovRatio, Cook's Distance, Mahalanobis, Hadi's measure, etc.)\nHadi & Simonoff (1993)\nKianifard & Swallow (1989)\nSebert & Montgomery & Rollier (1998)\nLeast Median of Squares \nLeast Trimmed Squares \nMinimum Volume Ellipsoid (MVE)\nMVE & LTS Plot \nBillor & Chatterjee & Hadi (2006)\nPena & Yohai (1995)\nSatman (2013)\nSatman (2015)\nSetan & Halim & Mohd (2000)\nLeast Absolute Deviations (LAD)\nQuantile Regression Parameter Estimation (quantileregression)\nLeast Trimmed Absolute Deviations (LTA)\nHadi (1992)\nMarchette & Solka (2003) Data Images\nSatman's GA based LTS estimation (2012)\nFischler & Bolles (1981) RANSAC Algorithm\nMinimum Covariance Determinant Estimator\nImon (2005) Algorithm\nBarratt & Angeris & Boyd (2020) CCF algorithm\nAtkinson (1994) Forward Search Algorithm\nBACON Algorithm (Billor & Hadi & Velleman (2000))\nHadi (1994) Algorithm\nChatterjee & Mächler (1997)\nTheil-Sen estimator for multiple regression\nDeepest Regression Estimator\nRobust Hat Matrix based Initial Subset Regressor\nSummary","category":"page"},{"location":"","page":"LinRegOutliers","title":"LinRegOutliers","text":"Algorithms","category":"page"},{"location":"#Installation","page":"LinRegOutliers","title":"Installation","text":"","category":"section"},{"location":"","page":"LinRegOutliers","title":"LinRegOutliers","text":"LinRegOutliers can be installed using the Julia REPL.  ","category":"page"},{"location":"","page":"LinRegOutliers","title":"LinRegOutliers","text":"julia> ]\n(@v1.8) pkg> add LinRegOutliers","category":"page"},{"location":"","page":"LinRegOutliers","title":"LinRegOutliers","text":"or","category":"page"},{"location":"","page":"LinRegOutliers","title":"LinRegOutliers","text":"julia> using Pgk\njulia> Pkg.add(\"LinRegOutliers\")","category":"page"},{"location":"","page":"LinRegOutliers","title":"LinRegOutliers","text":"then","category":"page"},{"location":"","page":"LinRegOutliers","title":"LinRegOutliers","text":"julia> using LinRegOutliers","category":"page"},{"location":"","page":"LinRegOutliers","title":"LinRegOutliers","text":"to make all the stuff be ready!","category":"page"},{"location":"#Examples","page":"LinRegOutliers","title":"Examples","text":"","category":"section"},{"location":"","page":"LinRegOutliers","title":"LinRegOutliers","text":"We provide some examples here.","category":"page"},{"location":"#Documentation","page":"LinRegOutliers","title":"Documentation","text":"","category":"section"},{"location":"","page":"LinRegOutliers","title":"LinRegOutliers","text":"Please check out the reference manual here.","category":"page"},{"location":"#News","page":"LinRegOutliers","title":"News","text":"","category":"section"},{"location":"","page":"LinRegOutliers","title":"LinRegOutliers","text":"We implemented algorithm(X, y) style calls for all of the algorithms where X is the design matrix and y is the response vector. \nWe implemented ~25 outlier detection algorithms which covers a high percentage of the literature.","category":"page"},{"location":"#Contributions","page":"LinRegOutliers","title":"Contributions","text":"","category":"section"},{"location":"","page":"LinRegOutliers","title":"LinRegOutliers","text":"You are probably the right contributor","category":"page"},{"location":"","page":"LinRegOutliers","title":"LinRegOutliers","text":"If you have statistics background\nIf you like Julia","category":"page"},{"location":"","page":"LinRegOutliers","title":"LinRegOutliers","text":"However, the second condition is more important because an outlier detection algorithm is just an algorithm. Reading the implemented methods is enough to implement new ones. Please follow the issues. Here is the a bunch of first shot introductions for new comers. Welcome and thank you in advance!","category":"page"},{"location":"#Citation","page":"LinRegOutliers","title":"Citation","text":"","category":"section"},{"location":"","page":"LinRegOutliers","title":"LinRegOutliers","text":"Please refer our original paper if you use the package in your research using","category":"page"},{"location":"","page":"LinRegOutliers","title":"LinRegOutliers","text":"Satman et al., (2021). LinRegOutliers: A Julia package for detecting outliers in linear regression. Journal of Open Source Software, 6(57), 2892, https://doi.org/10.21105/joss.02892","category":"page"},{"location":"","page":"LinRegOutliers","title":"LinRegOutliers","text":"or the bibtex entry","category":"page"},{"location":"","page":"LinRegOutliers","title":"LinRegOutliers","text":"@article{Satman2021,\n  doi = {10.21105/joss.02892},\n  url = {https://doi.org/10.21105/joss.02892},\n  year = {2021},\n  publisher = {The Open Journal},\n  volume = {6},\n  number = {57},\n  pages = {2892},\n  author = {Mehmet Hakan Satman and Shreesh Adiga and Guillermo Angeris and Emre Akadal},\n  title = {LinRegOutliers: A Julia package for detecting outliers in linear regression},\n  journal = {Journal of Open Source Software}\n}","category":"page"},{"location":"#Contact-and-Communication","page":"LinRegOutliers","title":"Contact & Communication","text":"","category":"section"},{"location":"","page":"LinRegOutliers","title":"LinRegOutliers","text":"Please use issues for a new feature request or bug reports.\nWe are in #linregoutliers channel on Julia Slack for any discussion requires online chatting. ","category":"page"},{"location":"algorithms/#Algorithms","page":"Algorithms","title":"Algorithms","text":"","category":"section"},{"location":"algorithms/#Hadi-and-Simonoff-(1993)","page":"Algorithms","title":"Hadi & Simonoff (1993)","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.hs93","category":"page"},{"location":"algorithms/#LinRegOutliers.HS93.hs93","page":"Algorithms","title":"LinRegOutliers.HS93.hs93","text":"hs93(setting; alpha = 0.05, basicsubsetindices = nothing)\n\nPerform the Hadi & Simonoff (1993) algorithm for the given regression setting.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\nalpha::Float64: Optional argument of the probability of rejecting the null hypothesis.\nbasicsubsetindices::Array{Int, 1}: Initial basic subset, by default, the algorithm creates an initial set of clean observations.\n\nDescription\n\nPerforms a forward search by selecting and enlarging an initial clean subset of observations and  iterates until scaled residuals exceeds a threshold.\n\nOutput\n\n[\"outliers\"]: Array of indices of outliers\n[\"t\"]: Threshold, specifically, calculated quantile of a Student-T distribution\n[\"d\"]: Internal and external scaled residuals. \n`[\"betas\"]: Vector of estimated regression coefficients.\n`[\"converged\"]: Boolean value indicating whether the algorithm converged or not.\n\nExamples\n\njulia> reg0001 = createRegressionSetting(@formula(calls ~ year), phones);\njulia> hs93(reg0001)\nDict{Any,Any} with 3 entries:\n  \"outliers\" => [14, 15, 16, 17, 18, 19, 20, 21]\n  \"t\"        => -3.59263\n  \"d\"        => [2.04474, 1.14495, -0.0633255, 0.0632934, -0.354349, -0.766818, -1.06862, -1.47638, -0.7…\n  \"converged\"=> true\n\nReferences\n\nHadi, Ali S., and Jeffrey S. Simonoff. \"Procedures for the identification of  multiple outliers in linear models.\" Journal of the American Statistical  Association 88.424 (1993): 1264-1272.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Kianifard-and-Swallow-(1989)","page":"Algorithms","title":"Kianifard & Swallow (1989)","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.ks89","category":"page"},{"location":"algorithms/#LinRegOutliers.KS89.ks89","page":"Algorithms","title":"LinRegOutliers.KS89.ks89","text":"ks89(setting; alpha = 0.05)\n\nPerform the Kianifard & Swallow (1989) algorithm for the given regression setting.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\nalpha::Float64: Optional argument of the probability of rejecting the null hypothesis.\n\nDescription\n\nThe algorithm starts with a clean subset of observations. This initial set is then enlarged  using recursive residuals. When the calculated statistics exceeds a threshold it terminates. \n\nOutput\n\n[\"outliers]: Array of indices of outliers.\n[\"betas\"]: Vector of regression coefficients.\n\nExamples\n\njulia> reg0001 = createRegressionSetting(@formula(stackloss ~ airflow + watertemp + acidcond), stackloss)\njulia> ks89(reg0001)\nDict{String, Vector} with 2 entries:\n  \"betas\"    => [-42.4531, 0.956605, 0.555571, -0.108766]\n  \"outliers\" => [4, 21]\n\nReferences\n\nKianifard, Farid, and William H. Swallow. \"Using recursive residuals, calculated on adaptively-ordered observations, to identify outliers in linear regression.\" Biometrics (1989): 571-585.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Sebert-and-Montgomery-and-Rollier-(1998)","page":"Algorithms","title":"Sebert & Montgomery & Rollier (1998)","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.smr98","category":"page"},{"location":"algorithms/#LinRegOutliers.SMR98.smr98","page":"Algorithms","title":"LinRegOutliers.SMR98.smr98","text":"smr98(setting)\n\nPerform the Sebert, Monthomery and Rollier (1998) algorithm for the given regression setting.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\n\nDescription\n\nThe algorithm starts with an ordinary least squares  estimation for a given model and data. Residuals and fitted responses are calculated  using the estimated model. A hierarchical clustering analysis is applied using standardized residuals and standardized fitted responses. The tree structure of clusters are cut using a threshold, e.g Majona criterion, as suggested by the authors. It is expected that  the subtrees with relatively small number of observations are declared to be clusters of outliers.\n\nOutput\n\n[\"outliers\"]: Array of indices of outliers.\n[\"betas\"]: Vector of regression coefficients.\n\nExamples\n\njulia> reg0001 = createRegressionSetting(@formula(calls ~ year), phones);\njulia> smr98(reg0001)\nDict{String, Vector} with 2 entries:\n  \"betas\"    => [-55.4519, 1.15692]\n  \"outliers\" => [15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n\nReferences\n\nSebert, David M., Douglas C. Montgomery, and Dwayne A. Rollier. \"A clustering algorithm for  identifying multiple outliers in linear regression.\" Computational statistics & data analysis  27.4 (1998): 461-484.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Least-Median-of-Squares","page":"Algorithms","title":"Least Median of Squares","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.lms","category":"page"},{"location":"algorithms/#LinRegOutliers.LMS.lms","page":"Algorithms","title":"LinRegOutliers.LMS.lms","text":"lms(setting; iters = nothing, crit = 2.5)\n\nPerform Least Median of Squares regression estimator with random sampling.\n\nArguments\n\nsetting::RegressionSetting: A regression setting object.\niters::Int: Number of random samples.\ncrit::Float64: Critical value for standardized residuals. \n\nDescription\n\nLMS (Least Median of Squares) estimator is highly robust with 50% breakdown property. The algorithm searches for regression coefficients which minimize (h)th ordered squared residual where h is Int(floor((n + 1.0) / 2.0))\n\nOutput\n\n[\"stdres\"]: Array of standardized residuals\n[\"S\"]: Standard error of regression\n[\"outliers\"]: Array of indices of outliers\n[\"objective\"]: LMS objective value\n[\"betas\"]: Estimated regression coefficients\n[\"crit\"]: Threshold value.\n\nExamples\n\njulia> reg = createRegressionSetting(@formula(calls ~ year), phones);\n\njulia> lms(reg)\nDict{Any,Any} with 6 entries:\n  \"stdres\"    => [2.28328, 1.55551, 0.573308, 0.608843, 0.220321, -0.168202, -0.471913, -0.860435, -0.31603, -0.110871  …  85.7265, 88.9849, 103.269, 116.705, 135.229, 159.69,…\n  \"S\"         => 1.17908\n  \"outliers\"  => [14, 15, 16, 17, 18, 19, 20, 21]\n  \"objective\" => 0.515348\n  \"betas\"      => [-56.1972, 1.1581]\n  \"crit\"      => 2.5\n\nReferences\n\nRousseeuw, Peter J. \"Least median of squares regression.\" Journal of the American  statistical association 79.388 (1984): 871-880.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Least-Trimmed-Squares","page":"Algorithms","title":"Least Trimmed Squares","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.lts","category":"page"},{"location":"algorithms/#LinRegOutliers.LTS.lts","page":"Algorithms","title":"LinRegOutliers.LTS.lts","text":"lts(setting; iters = nothing, crit = 2.5, earlystop = true)\n\nPerform the Fast-LTS (Least Trimmed Squares) algorithm for a given regression setting. \n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\niters::Int: Number of iterations.\ncrit::Float64: Critical value.\nearlystop::Bool: Early stop if the best objective does not change in iters / 2 iterations.\n\nDescription\n\nThe algorithm searches for estimations of regression parameters which minimize the sum of first h  ordered squared residuals where h is Int(floor((n + p + 1.0) / 2.0)). Specifically, our implementation,  uses the algorithm Fast-LTS in which concentration steps are used for enlarging a basic  subset to subset of clean observation of size h.  \n\nOutput\n\n[\"betas\"]: Estimated regression coefficients\n[\"S\"]: Standard error of regression\n[\"hsubset\"]: Best subset of clean observation of size h.\n[\"outliers\"]: Array of indices of outliers\n[\"scaled.residuals\"]: Array of scaled residuals\n[\"objective\"]: LTS objective value.\n\nExamples\n\njulia> reg = createRegressionSetting(@formula(calls ~ year), phones);\njulia> lts(reg)\nDict{Any,Any} with 6 entries:\n  \"betas\"            => [-56.5219, 1.16488]\n  \"S\"                => 1.10918\n  \"hsubset\"          => [11, 10, 5, 6, 23, 12, 13, 9, 24, 7, 3, 4, 8]\n  \"outliers\"         => [14, 15, 16, 17, 18, 19, 20, 21]\n  \"scaled.residuals\" => [2.41447, 1.63472, 0.584504, 0.61617, 0.197052, -0.222066, -0.551027, -0.970146, -0.397538, -0.185558  …  …\n  \"objective\"        => 3.43133\n\nReferences\n\nRousseeuw, Peter J., and Katrien Van Driessen. \"An algorithm for positive-breakdown  regression based on concentration steps.\" Data Analysis.  Springer, Berlin, Heidelberg, 2000. 335-346.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Minimum-Volume-Ellipsoid-(MVE)","page":"Algorithms","title":"Minimum Volume Ellipsoid (MVE)","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.mve","category":"page"},{"location":"algorithms/#LinRegOutliers.MVE.mve","page":"Algorithms","title":"LinRegOutliers.MVE.mve","text":"mve(data; alpha = 0.01)\n\nPerforms the Minimum Volume Ellipsoid algorithm for a robust covariance matrix.\n\nArguments\n\ndata::DataFrame: Multivariate data.\nalpha::Float64: Probability for quantiles of Chi-Squared statistic.\n\nDescription\n\nmve searches for a robust location vector and a robust scale matrix, e.g covariance matrix. The method also reports a usable diagnostic measure, Mahalanobis distances, which are calculated using  the robust counterparts instead of mean vector and usual covariance matrix. Mahalanobis distances  are directly comparible with quantiles of a ChiSquare Distribution with p degrees of freedom.\n\nOutput\n\n[\"goal\"]: Objective value\n[\"best.subset\"]: Indices of best h-subset of observations\n[\"robust.location\"]: Vector of robust location measures\n[\"robust.covariance\"]: Robust covariance matrix\n[\"squared.mahalanobis\"]: Array of Mahalanobis distances calculated using robust location and scale measures.\n[\"chisq.crit\"]: Chisquare quantile used in threshold\n[\"alpha\"]: Probability used in calculating the Chisquare quantile, e.g chisq.crit\n[\"outliers\"]: Array of indices of outliers.\n\nReferences\n\nVan Aelst, Stefan, and Peter Rousseeuw. \"Minimum volume ellipsoid.\" Wiley  Interdisciplinary Reviews: Computational Statistics 1.1 (2009): 71-82.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#MVE-and-LTS-Plot","page":"Algorithms","title":"MVE & LTS Plot","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.mveltsplot","category":"page"},{"location":"algorithms/#LinRegOutliers.MVELTSPlot.mveltsplot","page":"Algorithms","title":"LinRegOutliers.MVELTSPlot.mveltsplot","text":"mveltsplot(setting; alpha = 0.05, showplot = true)\n\nGenerate MVE - LTS plot for visual detecting of regression outliers.\n\nArguments\n\nsetting::RegressionSetting: A regression setting object.\nalpha::Float64: Probability for quantiles of Chi-Squared statistic.\nshowplot::Bool: Whether a plot is shown or only return statistics.\n\nDescription\n\nThis is a method of combination of lts and mve. Regression residuals and robust distances obtained  by mve and mve are used to generate a plot. Despite this is a visual method, drawing a plot is not really necessary. The algorithm divides the residuals-distances space into 4 parts, one for clean observations, one for vertical outliers (y-space outliers), one for bad-leverage points (x-space outliers), and one for  good leverage points (observations far from the remaining of data in both x and y space).   \n\nOutput\n\n[\"plot\"]: Generated plot object\n[\"robust.distances\"]: Robust Mahalanobis distances \n[\"scaled.residuals\"]: Scaled residuals of an lts estimate\n[\"chi.squared\"]: Quantile of Chi-Squared distribution \n[\"regular.points\"]: Array of indices of clean observations\n[\"outlier.points\"]: Array of indices of y-space outliers (vertical outliers)\n[\"leverage.points\"]: Array of indices of x-space outliers (bad leverage points)\n[\"outlier.and.leverage.points\"]: Array of indices of xy-space outliers (good leverage points)\n\nReferences\n\nVan Aelst, Stefan, and Peter Rousseeuw. \"Minimum volume ellipsoid.\" Wiley  Interdisciplinary Reviews: Computational Statistics 1.1 (2009): 71-82.\n\nwarning: Dependencies\nThis method is enabled when the Plots package is installed and loaded.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Billor-and-Chatterjee-and-Hadi-(2006)","page":"Algorithms","title":"Billor & Chatterjee & Hadi (2006)","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.bch","category":"page"},{"location":"algorithms/#LinRegOutliers.BCH.bch","page":"Algorithms","title":"LinRegOutliers.BCH.bch","text":"bch(setting; alpha = 0.05, maxiter = 1000, epsilon = 0.000001)\n\nPerform the Billor & Chatterjee & Hadi (2006) algorithm for the given regression setting.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\nalpha::Float64: Optional argument of the probability of rejecting the null hypothesis.\nmaxiter::Int: Maximum number of iterations for calculating iterative weighted least squares estimates.\nepsilon::Float64: Accuracy for determining convergency.\n\nDescription\n\nThe algorithm initially constructs a basic subset. These basic subset is then used to  generate initial weights for a iteratively least squares estimation. Regression coefficients obtained  in this stage are robust regression estimates. Squared normalized distances and squared normalized  residuals are used in bchplot which serves a visual way for investigation of outliers and their  properties.\n\nOutput\n\n[\"betas\"]: Final estimate of regression coefficients                               \n[\"squared.normalized.robust.distances\"]:  \n[\"weights\"]: Final weights used in calculation of WLS estimates                             \n[\"outliers\"]: Array of indices of outliers\n[\"squared.normalized.residuals\"]: Array of squared normalized residuals\n[\"residuals\"]: Array of regression residuals\n[\"basic.subset\"]: Array of indices of basic subset.\n\nExamples\n\njulia> reg  = createRegressionSetting(@formula(calls ~ year), phones);\njulia> Dict{Any,Any} with 7 entries:\n\"betas\"                               => [-55.9205, 1.15572]\n\"squared.normalized.robust.distances\" => [0.104671, 0.0865052, 0.0700692, 0.0553633, 0.0423875, 0.03…\n\"weights\"                             => [0.00186158, 0.00952088, 0.0787321, 0.0787321, 0.0787321, 0…\n\"outliers\"                            => [1, 14, 15, 16, 17, 18, 19, 20, 21]\n\"squared.normalized.residuals\"        => [5.53742e-5, 2.42977e-5, 2.36066e-6, 2.77706e-6, 1.07985e-7…\n\"residuals\"                           => [2.5348, 1.67908, 0.523367, 0.567651, 0.111936, -0.343779, …\n\"basic.subset\"                        => [1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  15, 16, 17, 18, 19, 20, …\n\nReferences\n\nBillor, Nedret, Samprit Chatterjee, and Ali S. Hadi. \"A re-weighted least squares method  for robust regression estimation.\" American journal of mathematical and management sciences 26.3-4 (2006): 229-252.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Pena-and-Yohai-(1995)","page":"Algorithms","title":"Pena & Yohai (1995)","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.py95","category":"page"},{"location":"algorithms/#LinRegOutliers.PY95.py95","page":"Algorithms","title":"LinRegOutliers.PY95.py95","text":"py95(setting)\n\nPerform the Pena & Yohai (1995) algorithm for the given regression setting.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\n\nDescription\n\nThe algorithm starts by constructing an influence matrix using results  of an ordinary least squares estimate for a given model and data. In the second stage,  the eigen structure of the influence matrix is examined for detecting suspected subsets of data.\n\nOutput\n\n[\"outliers\"]: Array of indices of outliers\n[\"suspected.sets\"]: Arrays of indices of observations for corresponding eigen value of the influence matrix.\n[\"betas]: Vector of estimated regression coefficients using the clean observations.\n\nExamples\n\njulia> reg0001 = createRegressionSetting(@formula(y ~ x1 + x2 + x3), hbk);\njulia> py95(reg0001)\nict{Any,Any} with 2 entries:\n  \"outliers\"       => [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n  \"suspected.sets\" => Set([[14, 13], [43, 54, 24, 38, 22], [6, 10], [14, 7, 8, 3, 10, 2, 5, 6, 1, 9, 4…\n\nReferences\n\nPeña, Daniel, and Victor J. Yohai. \"The detection of influential subsets in linear  regression by using an influence matrix.\" Journal of the Royal Statistical Society:  Series B (Methodological) 57.1 (1995): 145-156.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Satman-(2013)","page":"Algorithms","title":"Satman (2013)","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.satman2013","category":"page"},{"location":"algorithms/#LinRegOutliers.Satman2013.satman2013","page":"Algorithms","title":"LinRegOutliers.Satman2013.satman2013","text":"satman2013(setting)\n\nPerform Satman (2013) algorithm for the given regression setting.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\n\nDescription\n\nThe algorithm constructs a fast and robust covariance matrix to calculate robust mahalanobis distances. These distances are then used to construct weights for later use in a weighted least  squares estimation. In the last stage, C-steps are iterated on the basic subset found in previous stages. \n\nOutput\n\n[\"outliers\"]: Array of indices of outliers.\n[\"betas\"]: Array of estimated regression coefficients.\n[\"residuals\"]: Array of residuals.\n\nExamples\n\njulia> eg0001 = createRegressionSetting(@formula(y ~ x1 + x2 + x3), hbk);\njulia> satman2013(reg0001)\nDict{Any,Any} with 1 entry:\n  \"outliers\" => [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 47]\n  \"betas\" => ...\n  \"residuals\" => ...\n\nReferences\n\nSatman, Mehmet Hakan. \"A new algorithm for detecting outliers in linear regression.\"  International Journal of statistics and Probability 2.3 (2013): 101.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Satman-(2015)","page":"Algorithms","title":"Satman (2015)","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.satman2015","category":"page"},{"location":"algorithms/#LinRegOutliers.Satman2015.satman2015","page":"Algorithms","title":"LinRegOutliers.Satman2015.satman2015","text":"satman2015(setting)\n\nPerform Satman (2015) algorithm for the given regression setting.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\n\nDescription\n\nThe algorithm starts with sorting the design matrix using the Non-dominated sorting algorithm. An initial basic subset is then constructed using the ranks obtained in previous stage. After many  C-steps, observations with high standardized residuals are reported to be outliers.\n\nOutput\n\n[\"outliers]\": Array of indices of outliers.\n[betas]: Array of regression coefficients.\n[residuals]: Array of residuals.\n[standardized_residuals]: Array of standardized residuals.\n\nExamples\n\njulia> eg0001 = createRegressionSetting(@formula(y ~ x1 + x2 + x3), hbk);\njulia> satman2015(reg0001)\nDict{Any,Any} with 1 entry:\n  \"outliers\" => [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 14, 47]\n\n\nReferences\n\nSatman, Mehmet Hakan. \"Fast online detection of outliers using least-trimmed squares  regression with non-dominated sorting based initial subsets.\"  International Journal of Advanced Statistics and Probability 3.1 (2015): 53.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"## Setan & Halim & Mohd (2000)","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.asm2000","category":"page"},{"location":"algorithms/#LinRegOutliers.ASM2000.asm2000","page":"Algorithms","title":"LinRegOutliers.ASM2000.asm2000","text":"asm2000(setting)\n\nPerform the Setan, Halim and Mohd (2000) algorithm for the given regression setting.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\n\nDescription\n\nThe algorithm performs a Least Trimmed Squares (LTS) estimate and yields standardized  residual - fitted response pairs. A single linkage clustering algorithm is performed on these pairs. Like smr98, the cluster tree is cut using the Majona criterion. Subtrees with  relatively small number of observations are declared to be outliers.\n\nOutput\n\n[\"outliers\"]: Vector of indices of outliers.\n[\"betas\"]: Vector of regression coefficients.\n\nExamples\n\njulia> reg0001 = createRegressionSetting(@formula(calls ~ year), phones);\njulia> asm2000(reg0001)\nDict{Any, Any} with 2 entries:\n  \"betas\"    => [-63.4816, 1.30406]\n  \"outliers\" => [15, 16, 17, 18, 19, 20]\n\nReferences\n\nRobiah Adnan, Mohd Nor Mohamad, & Halim Setan (2001).  Identifying multiple outliers in linear regression: robust fit and clustering approach.  Proceedings of the Malaysian Science and Technology Congress 2000: Symposium C, Vol VI, (p. 400).  Malaysia: Confederation of Scientific and Technological Associations in Malaysia COSTAM.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Least-Absolute-Deviations-(LAD)","page":"Algorithms","title":"Least Absolute Deviations (LAD)","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.lad","category":"page"},{"location":"algorithms/#LinRegOutliers.LAD.lad","page":"Algorithms","title":"LinRegOutliers.LAD.lad","text":"lad(setting; exact = true)\n\nPerform Least Absolute Deviations regression for a given regression setting.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\nexact::Bool: If true, use exact LAD regression. If false, estimate LAD regression parameters using GA. Default is true.\n\nDescription\n\nThe LAD estimator searches for regression the parameters estimates that minimize the sum of absolute residuals. The optimization problem is \n\nMin z = u1(-) + u1(+) + u2(-) + u2(+) + .... + un(-) + un(+) Subject to:     y1 - beta0 - beta1 * x2 + u1(-) - u1(+) = 0     y2 - beta0 - beta1 * x2 + u2(-) - u2(+) = 0     .     .     .     yn - beta0 - beta1 * xn + un(-) - un(+) = 0 where      ui(-), ui(+) >= 0     i = 1, 2, ..., n      beta0, beta1 in R      n : Number of observations \n\nOutput\n\n[\"betas\"]: Estimated regression coefficients\n[\"residuals\"]: Regression residuals\n[\"model\"]: Linear Programming Model\n\nExamples\n\njulia> reg0001 = createRegressionSetting(@formula(calls ~ year), phones);\njulia> lad(reg0001)\nDict{Any,Any} with 2 entries:\n  \"betas\"     => [-57.3269, 1.19155]\n  \"residuals\" => [2.14958, 1.25803, 0.0664872, 0.0749413, -0.416605, -0.90815, -1.2997, -1.79124,…\n\n\n\n\n\n\nlad(X, y, exact = true)\n\nPerform Least Absolute Deviations regression for a given regression setting.\n\nArguments\n\nX::AbstractMatrix{Float64}: Design matrix of the linear model.\ny::AbstractVector{Float64}: Response vector of the linear model.\nexact::Bool: If true, use exact LAD regression. If false, estimate LAD regression parameters using GA. Default is true.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Least-Trimmed-Absolute-Deviations-(LTA)","page":"Algorithms","title":"Least Trimmed Absolute Deviations (LTA)","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.lta","category":"page"},{"location":"algorithms/#LinRegOutliers.LTA.lta","page":"Algorithms","title":"LinRegOutliers.LTA.lta","text":"lta(setting; exact = false, earlystop = true)\n\nPerform the Hawkins & Olive (1999) algorithm (Least Trimmed Absolute Deviations)  for the given regression setting.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\nexact::Bool: Consider all possible subsets of p or not where p is the number of regression parameters.\nearlystop::Bool: Early stop if the best objective does not change in number of remaining iters / 5 iterations.\n\nDescription\n\nlta is a trimmed version of lad in which the sum of first h absolute residuals is minimized where h is Int(floor((n + p + 1.0) / 2.0)). \n\nOutput\n\n[\"betas\"]: Estimated regression coefficients\n[\"objective]: Objective value\n\nExamples\n\njulia> reg0001 = createRegressionSetting(@formula(calls ~ year), phones);\njulia> lta(reg0001)\nDict{Any,Any} with 2 entries:\n  \"betas\"     => [-55.5, 1.15]\n  \"objective\" => 5.7\n\njulia> lta(reg0001, exact = true)\nDict{Any,Any} with 2 entries:\n  \"betas\"     => [-55.5, 1.15]\n  \"objective\" => 5.7  \n\nReferences\n\nHawkins, Douglas M., and David Olive. \"Applications and algorithms for least trimmed sum of  absolute deviations regression.\" Computational Statistics & Data Analysis 32.2 (1999): 119-134.\n\n\n\n\n\nlta(X, y; exact = false)\n\nPerform the Hawkins & Olive (1999) algorithm (Least Trimmed Absolute Deviations)  for the given regression setting.\n\nArguments\n\nX::AbstractMatrix{Float64}: Design matrix of linear regression model.\ny::AbstractVector{Float64}: Response vector of linear regression model.\nexact::Bool: Consider all possible subsets of p or not where p is the number of regression parameters.\nearlystop::Bool: Early stop if the best objective does not change in number of remaining iters / 5 iterations.\n\nReferences\n\nHawkins, Douglas M., and David Olive. \"Applications and algorithms for least trimmed sum of  absolute deviations regression.\" Computational Statistics & Data Analysis 32.2 (1999): 119-134.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Hadi-(1992)","page":"Algorithms","title":"Hadi (1992)","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.hadi1992","category":"page"},{"location":"algorithms/#LinRegOutliers.Hadi92.hadi1992","page":"Algorithms","title":"LinRegOutliers.Hadi92.hadi1992","text":"hadi1992(multivariateData)\n\nPerform Hadi (1992) algorithm for a given multivariate data. \n\nArguments\n\nmultivariateData::AbstractMatrix{Float64}: Multivariate data.\n\nDescription\n\nAlgorithm starts with an initial subset and enlarges the subset to  obtain robust covariance matrix and location estimates. \n\nOutput\n\n[\"outliers\"]: Array of indices of outliers\n[\"critical.chi.squared\"]: Threshold value for determining being an outlier\n[\"rth.robust.distance\"]: rth robust distance, where (r+1)th robust distance is the first one that exceeds the threshold.\n\nExamples\n\njulia> multidata = hcat(hbk.x1, hbk.x2, hbk.x3);\n\njulia> hadi1992(multidata)\nDict{Any,Any} with 3 entries:\n  \"outliers\"              => [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n  \"critical.chi.squared\" => 7.81473\n  \"rth.robust.distance\"   => 5.04541\n\n# Reference Hadi, Ali S. \"Identifying multiple outliers in multivariate data.\"  Journal of the Royal Statistical Society: Series B (Methodological) 54.3 (1992): 761-771.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Marchette-and-Solka-(2003)-Data-Images","page":"Algorithms","title":"Marchette & Solka (2003) Data Images","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.dataimage","category":"page"},{"location":"algorithms/#LinRegOutliers.DataImage.dataimage","page":"Algorithms","title":"LinRegOutliers.DataImage.dataimage","text":"dataimage(dataMatrix; distance = :)\n\nGenerate the Marchette & Solka (2003) data image for a given data matrix. \n\nArguments\n\ndataMatrix::AbstractVector{Float64}: Data matrix with dimensions n x p, where n is the number of observations and p is the number of variables.\ndistance::Symbol: Optional argument for the distance function.\n\nNotes\n\ndistance is :mahalanobis by default, for the Mahalanobis distances. \nuse \n\n    dataimage(mat, distance = :euclidean)\n\nto use Euclidean distances.\n\nExamples\n\njulia> x1 = hbk[:,\"x1\"];\njulia> x2 = hbk[:,\"x2\"];\njulia> x3 = hbk[:,\"x3\"];\njulia> mat = hcat(x1, x2, x3);\njulia> di = dataimage(mat, distance = :euclidean)\njulia> Plots.plot(di)\n\nReferences\n\nMarchette, David J., and Jeffrey L. Solka. \"Using data images for outlier detection.\"  Computational Statistics & Data Analysis 43.4 (2003): 541-552.\n\nwarning: Dependencies\nThis method is enabled when the Plots package is installed and loaded.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Satman's-GA-based-LTS-estimation-(2012)","page":"Algorithms","title":"Satman's GA based LTS estimation (2012)","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.galts","category":"page"},{"location":"algorithms/#LinRegOutliers.GALTS.galts","page":"Algorithms","title":"LinRegOutliers.GALTS.galts","text":"galts(setting)\n\nPerform Satman(2012) algorithm for estimating LTS coefficients.\n\nArguments\n\nsetting: A regression setting object.\n\nDescription\n\nThe algorithm performs a genetic search for estimating LTS coefficients using C-Steps. \n\nOutput\n\n[\"betas\"]: Robust regression coefficients\n[\"best.subset\"]: Clean subset of h observations, where h is an integer greater than n / 2. The default value of h is Int(floor((n + p + 1.0) / 2.0)).\n[\"objective\"]: Objective value\n\nExamples\n\njulia> reg = createRegressionSetting(@formula(calls ~ year), phones);\njulia> galts(reg)\nDict{Any,Any} with 3 entries:\n  \"betas\"       => [-56.5219, 1.16488]\n  \"best.subset\" => [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 23, 24]\n  \"objective\"   => 3.43133\n\nReferences\n\nSatman, M. Hakan. \"A genetic algorithm based modification on the lts algorithm for large data sets.\"  Communications in Statistics-Simulation and Computation 41.5 (2012): 644-652.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Fischler-and-Bolles-(1981)-RANSAC-Algorithm","page":"Algorithms","title":"Fischler & Bolles (1981) RANSAC Algorithm","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.ransac","category":"page"},{"location":"algorithms/#LinRegOutliers.Ransac.ransac","page":"Algorithms","title":"LinRegOutliers.Ransac.ransac","text":"ransac(setting; t, w=0.5, m=0, k=0, d=0, confidence=0.99)\n\nRun the RANSAC (1981) algorithm for the given regression setting\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and a dataset.\nt::Float64: The threshold distance of a sample point to the regression hyperplane to determine if it fits the model well.\nw::Float64: The probability of a sample point being inlier, default=0.5.\nm::Int: The number of points to sample to estimate the model parameter for each iteration. If set to 0, defaults to picking p points which is the minimum required.\nk::Int: The number of iterations to run. If set to 0, is calculated according to the formula given in the paper based on outlier probability and the sample set size.\nd::Int: The number of close data points required to accept the model. Defaults to number of data points multiplied by inlier ratio.\nconfidence::Float64: Required to determine the number of optimum iterations if k is not specified.\n\nOutput\n\n[\"outliers\"]: Array of indices of outliers.\n\nExamples\n\njulia> df = DataFrame(y=[0,1,2,3,3,4,10], x=[0,1,2,2,3,4,2])\njulia> reg = createRegressionSetting(@formula(y ~ x), df)\njulia> ransac(reg, t=0.8, w=0.85)\nDict{String,Array{Int64,1}} with 1 entry:\n  \"outliers\" => [7]\n\nReferences\n\nMartin A. Fischler & Robert C. Bolles (June 1981). \"Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography\" Comm. ACM. 24 (6): 381–395.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Minimum-Covariance-Determinant-Estimator-(MCD)","page":"Algorithms","title":"Minimum Covariance Determinant Estimator (MCD)","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.mcd","category":"page"},{"location":"algorithms/#LinRegOutliers.MVE.mcd","page":"Algorithms","title":"LinRegOutliers.MVE.mcd","text":"mcd(data; alpha = 0.01)\n\nPerforms the Minimum Covariance Determinant algorithm for a robust covariance matrix.\n\nArguments\n\ndata::DataFrame: Multivariate data.\nalpha::Float64: Probability for quantiles of Chi-Squared statistic.\n\nDescription\n\nmcd searches for a robust location vector and a robust scale matrix, e.g covariance matrix. The method also reports a usable diagnostic measure, Mahalanobis distances, which are calculated using  the robust counterparts instead of mean vector and usual covariance matrix. Mahalanobis distances  are directly comparible with quantiles of a ChiSquare Distribution with p degrees of freedom.  \n\nOutput\n\n[\"goal\"]: Objective value\n[\"best.subset\"]: Indices of best h-subset of observations\n[\"robust.location\"]: Vector of robust location measures\n[\"robust.covariance\"]: Robust covariance matrix\n[\"squared.mahalanobis\"]: Array of Mahalanobis distances calculated using robust location and scale measures.\n[\"chisq.crit\"]: Chisquare quantile used in threshold\n[\"alpha\"]: Probability used in calculating the Chisquare quantile, e.g chisq.crit\n[\"outliers\"]: Array of indices of outliers.\n\nNotes\n\nAlgorithm is implemented using concentration steps as described in the reference paper. However, details about number of iterations are slightly different.\n\nReferences\n\nRousseeuw, Peter J., and Katrien Van Driessen. \"A fast algorithm for the minimum covariance  determinant estimator.\" Technometrics 41.3 (1999): 212-223.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Imon-(2005)-Algorithm","page":"Algorithms","title":"Imon (2005) Algorithm","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.imon2005","category":"page"},{"location":"algorithms/#LinRegOutliers.Imon2005.imon2005","page":"Algorithms","title":"LinRegOutliers.Imon2005.imon2005","text":"imon2005(setting)\n\nPerform the Imon 2005 algorithm for a given regression setting.\n\nArguments\n\nsetting::RegressionSetting: A regression setting.\n\nDescription\n\nThe algorithm estimates the GDFFITS diagnostic, which is an extension of well-known regression  diagnostic DFFITS. Unlikely, GDFFITS is used for detecting multiple outliers whereas the original one was used for detecting single outliers. \n\nOutput\n\n[\"crit\"]: The critical value used\n[\"gdffits\"]: Array of GDFFITS diagnostic calculated for observations\n[\"outliers\"]: Array of indices of outliers.\n[\"betas\"]: Vector of regression coefficients.\n\nNotes\n\nThe implementation uses LTS rather than LMS as suggested in the paper. \n\nReferences\n\nA. H. M. Rahmatullah Imon (2005) Identifying multiple influential observations in linear regression,  Journal of Applied Statistics, 32:9, 929-946, DOI: 10.1080/02664760500163599\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Barratt-and-Angeris-and-Boyd-(2020)-CCF-algorithm","page":"Algorithms","title":"Barratt & Angeris & Boyd (2020) CCF algorithm","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.ccf","category":"page"},{"location":"algorithms/#LinRegOutliers.CCF.ccf","page":"Algorithms","title":"LinRegOutliers.CCF.ccf","text":"ccf(setting; starting_lambdas = nothing)\n\nPerform signed gradient descent for clipped convex functions for a given regression setting.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\nstarting_lambdas::AbstractVector{Float64}: Starting values of weighting parameters used by signed gradient descent.\nalpha::Float64: Loss at which a point is labeled as an outlier (points with loss ≥ alpha will be called outliers).\nmax_iter::Int64: Maximum number of iterations to run signed gradient descent.\nbeta::Float64: Step size parameter.\ntol::Float64: Tolerance below which convergence is declared.\n\nOutput\n\n[\"betas\"]: Robust regression coefficients\n[\"\"outliers\"]: Array of indices of outliers\n[\"\"lambdas\"]: Lambda coefficients estimated in each iteration \n[\"\"residuals\"]: Regression residuals.\n\nExamples\n\njulia> reg0001 = createRegressionSetting(@formula(calls ~ year), phones);\njulia> ccf(reg0001)\nDict{Any,Any} with 4 entries:\n  \"betas\"     => [-63.4816, 1.30406]\n  \"outliers\"  => [15, 16, 17, 18, 19, 20]\n  \"lambdas\"   => [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  2.77556e-17, 2.77556e-17, 0…\n  \"residuals\" => [-2.67878, -1.67473, -0.37067, -0.266613, 0.337444, 0.941501, 1.44556, 2.04962, 1…\n\n\nReferences\n\nBarratt, S., Angeris, G. & Boyd, S. Minimizing a sum of clipped convex functions. Optim Lett 14, 2443–2459 (2020). https://doi.org/10.1007/s11590-020-01565-4\n\n\n\n\n\nccf(X, y; starting_lambdas = nothing)\n\nPerform signed gradient descent for clipped convex functions for a given regression setting.\n\nArguments\n\nX::AbstractMatrix{Float64}: Design matrix of the linear model.\ny::AbstractVector{Float64}: Response vector of the linear model.\nstarting_lambdas::AbstractVector{Float64}: Starting values of weighting parameters used by signed gradient descent.\nalpha::Float64: Loss at which a point is labeled as an outlier. If unspecified, will be chosen as p*mean(residuals.^2), where residuals are OLS residuals.\np::Float64: Points that have squared OLS residual greater than p times the mean squared OLS residual are considered outliers.\nmax_iter::Int64: Maximum number of iterations to run signed gradient descent.\nbeta::Float64: Step size parameter.\ntol::Float64: Tolerance below which convergence is declared.\n\nOutput\n\n[\"betas\"]: Robust regression coefficients\n[\"\"outliers\"]: Array of indices of outliers\n[\"\"lambdas\"]: Lambda coefficients estimated in each iteration \n[\"\"residuals\"]: Regression residuals.\n\nReferences\n\nBarratt, S., Angeris, G. & Boyd, S. Minimizing a sum of clipped convex functions. Optim Lett 14, 2443–2459 (2020). https://doi.org/10.1007/s11590-020-01565-4\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Atkinson-(1994)-Forward-Search-Algorithm","page":"Algorithms","title":"Atkinson (1994) Forward Search Algorithm","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.atkinson94","category":"page"},{"location":"algorithms/#LinRegOutliers.Atkinson94.atkinson94","page":"Algorithms","title":"LinRegOutliers.Atkinson94.atkinson94","text":"    atkinson94(setting, iters, crit)\n\nRuns the Atkinson94 algorithm to find out outliers using LMS method.\n\nArguments\n\nsetting::RegressionSetting: A regression setting object.\niters::Int: Number of random samples.\ncrit::Float64: Critical value for residuals\n\nDescription\n\nThe algorithm randomly selects initial basic subsets and performs a very robust method, e.g lms to enlarge the basic subset. In each iteration of forward search, the best objective value and parameter  estimates are stored. These values are also used in Atkinson's Stalactite Plot for a visual investigation of  outliers. See atkinsonstalactiteplot.\n\nOutput\n\n[\"optimum_index\"]: The iteration number in which the minimum objective is obtained\n[\"residuals_matrix\"]: Matrix of residuals obtained in each iteration\n[\"outliers\"]: Array of indices of detected outliers\n[\"objective\"]: Minimum objective value\n[\"coef\"]: Estimated regression coefficients\n[\"crit\"]: Critical value given by the user.\n\nExamples\n\njulia> reg = createRegressionSetting(@formula(stackloss ~ airflow + watertemp + acidcond), stackloss)\njulia> atkinson94(reg)\nDict{Any,Any} with 6 entries:\n  \"optimum_index\"    => 10\n  \"residuals_matrix\" => [0.0286208 0.0620609 … 0.0796249 0.0; 0.0397778 0.120547 … 0.118437 0.0397778; … ; 1.21133 1.80846 … 0.690327 4.14366; 1.61977 0.971592 … 0.616204 3.58098]\n  \"outliers\"         => [1, 3, 4, 21]\n  \"objective\"        => 0.799134\n  \"coef\"             => [-38.3133, 0.745659, 0.432794, 0.0104587]\n  \"crit\"             => 3.0\n\n\nReferences\n\nAtkinson, Anthony C. \"Fast very robust methods for the detection of multiple outliers.\" Journal of the American Statistical Association 89.428 (1994): 1329-1339.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#BACON-Algorithm-(Billor-and-Hadi-and-Velleman-(2000))","page":"Algorithms","title":"BACON Algorithm (Billor & Hadi & Velleman (2000))","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.bacon","category":"page"},{"location":"algorithms/#LinRegOutliers.Bacon.bacon","page":"Algorithms","title":"LinRegOutliers.Bacon.bacon","text":"    bacon(setting, m, method, alpha)\n\nRun the BACON algorithm to detect outliers on regression data.\n\nArguments:\n\nsetting: RegressionSetting object with a formula and a dataset.\nm: The number of elements to be included in the initial subset.\nmethod: The distance method to use for selecting the points for initial subset\nalpha: The quantile used for cutoff\n\nDescription\n\nBACON (Blocked Adaptive Computationally efficient Outlier Nominators) algoritm, defined in the citation below, has many versions, e.g BACON for multivariate data, BACON for regression etc. Since the design matrix of a regression model is multivariate data, BACON for multivariate data is performed in early stages of the algorithm. After selecting a clean subset of observations, then a forward search is applied. Observations with high studendized residuals are reported as outliers.\n\nOutput\n\n[\"outliers\"]: Array of indices of outliers.\n[\"betas\"]: Array of estimated coefficients.\n\nExamples\n\njulia> reg = createRegressionSetting(@formula(stackloss ~ airflow + watertemp + acidcond), stackloss)\njulia> bacon(reg, m=12)\nDict{String, Vector} with 2 entries:\n  \"betas\"    => [-37.6525, 0.797686, 0.57734, -0.0670602]\n  \"outliers\" => [1, 3, 4, 21]\n\nReferences\n\nBillor, Nedret, Ali S. Hadi, and Paul F. Velleman. \"BACON: blocked adaptive computationally efficient outlier nominators.\" Computational statistics & data analysis 34.3 (2000): 279-298.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Hadi-(1994)-Algorithm","page":"Algorithms","title":"Hadi (1994) Algorithm","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.hadi1994","category":"page"},{"location":"algorithms/#LinRegOutliers.Hadi94.hadi1994","page":"Algorithms","title":"LinRegOutliers.Hadi94.hadi1994","text":"hadi1994(multivariateData)\n\nPerform Hadi (1994) algorithm for a given multivariate data.\n\nArguments\n\nmultivariateData::AbstractMatrix{Float64}: Multivariate data.\n\nDescription\n\nAlgorithm starts with an initial subset and enlarges the subset to  obtain robust covariance matrix and location estimates. This algorithm  is an extension of hadi1992.\n\nOutput\n\n[\"outliers\"]: Array of indices of outliers\n[\"critical.chi.squared\"]: Threshold value for determining being an outlier\n[\"rth.robust.distance\"]: rth robust distance, where (r+1)th robust distance is the first one that exceeds the threshold.\n\nExamples\n\njulia> multidata = hcat(hbk.x1, hbk.x2, hbk.x3);\n\njulia> hadi1994(multidata)\nDict{Any,Any} with 3 entries:\n  \"outliers\"              => [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n  \"critical.chi.squared\" => 7.81473\n  \"rth.robust.distance\"   => 5.04541\n\n# Reference Hadi, Ali S. \"A modification of a method for the dedection of outliers in multivariate samples\" Journal of the Royal Statistical Society: Series B (Methodological) 56.2 (1994): 393-396.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Chatterjee-and-Mächler-(1997)","page":"Algorithms","title":"Chatterjee & Mächler (1997)","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.cm97","category":"page"},{"location":"algorithms/#LinRegOutliers.CM97.cm97","page":"Algorithms","title":"LinRegOutliers.CM97.cm97","text":"cm97(setting; maxiter = 1000)\n\nPerform the Chatterjee and Mächler (1997) algorithm for the given regression setting.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\n\nDescription\n\nThe algorithm performs a iteratively weighted least squares estimation to obtain robust regression coefficients.\n\nOutput\n\n[\"betas\"]: Robust regression coefficients\n[\"iterations\"]: Number of iterations performed\n[\"converged\"]: true if the algorithm converges, otherwise, false.\n\nExamples\n\njulia> myreg = createRegressionSetting(@formula(stackloss ~ airflow + watertemp + acidcond), stackloss)\njulia> result = cm97(myreg)\nDict{String,Any} with 3 entries:\n  \"betas\"      => [-37.0007, 0.839285, 0.632333, -0.113208]\n  \"iterations\" => 22\n  \"converged\"  => true\n\nReferences\n\nChatterjee, Samprit, and Martin Mächler. \"Robust regression: A weighted least squares approach.\"  Communications in Statistics-Theory and Methods 26.6 (1997): 1381-1394.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Quantile-Regression","page":"Algorithms","title":"Quantile Regression","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.quantileregression","category":"page"},{"location":"algorithms/#LinRegOutliers.QuantileRegression.quantileregression","page":"Algorithms","title":"LinRegOutliers.QuantileRegression.quantileregression","text":"quantileregression(setting; tau = 0.5)\n\nPerform Quantile Regression for a given regression setting (multiple linear regression).\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\ntau::Float64: Quantile level. Default is 0.5.\n\nDescription\n\nThe Quantile Regression estimator searches for the regression parameter estimates that minimize the \n\nMin z = (1 - tau) (u1(-) + u2(-) + ... + un(-)) + tau (u1(+) + u2(+) + ... + un(+)) Subject to:     y1 - beta0 - beta1 * x2 + u1(-) - u1(+) = 0     y2 - beta0 - beta1 * x2 + u2(-) - u2(+) = 0     .     .     .     yn - beta0 - beta1 * xn + un(-) - un(+) = 0 where      ui(-), ui(+) >= 0     i = 1, 2, ..., n      beta0, beta1 in R      n : Number of observations      model is the y = beta1 + beta2 * x + u \n\nOutput\n\n[\"betas\"]: Estimated regression coefficients\n[\"residuals\"]: Regression residuals\n[\"model\"]: Linear Programming Model\n\nExamples\n\njulia> reg0001 = createRegressionSetting(@formula(calls ~ year), phones);\njulia> quantileregression(reg0001)\n\n\n\n\n\nquantileregression(X, y, tau = 0.5)\n\nEstimates parameters of linear regression using Quantile Regression Estimator for a given regression setting.\n\nArguments\n\nX::AbstractMatrix{Float64}: Design matrix of the linear model.\ny::AbstractVector{Float64}: Response vector of the linear model.\ntau::Float64: Quantile level. Default is 0.5.\n\nExamples\n\njulia> income = [420.157651, 541.411707, 901.157457, 639.080229, 750.875606];\njulia> foodexp = [255.839425, 310.958667, 485.680014, 402.997356, 495.560775];\n\njulia> n = length(income)\njulia> X = hcat(ones(Float64, n), income)\n\njulia> result = quantileregression(X, foodexp, tau = 0.25)\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Theil-Sen-estimator-for-multiple-regresion","page":"Algorithms","title":"Theil-Sen estimator for multiple regresion","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.theilsen","category":"page"},{"location":"algorithms/#LinRegOutliers.TheilSen.theilsen","page":"Algorithms","title":"LinRegOutliers.TheilSen.theilsen","text":"theilsen(setting, m, nsamples = 5000)\n\nTheil-Sen estimator for multiple regression. \n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\nm::Int: Number of observations to be used in each iteration. This number must be in the range [p, n], where p is the number of regressors and n is the number of observations.\nnsamples::Int: Number of m-samples. Default is 5000.\n\nDescription\n\nThe function starts with a regression formula and datasets. The number of observations to be used in each iteration  is specified by the user. The function then randomly selects m observations from the dataset and performs an ordinary  least squares estimation. The estimated coefficients are saved. The process is repeated until nsamples regressions are estimated.  The multivariate median of the estimated coefficients is then calculated. In this case, the multivariate median is the point that minimizes the sum of distances to all the estimated coefficients. Hooke & Jeeves algorithm is used for the  optimization problem. \n\nReferences\n\nDang, X., Peng, H., Wang, X., & Zhang, H. (2008). Theil-sen estimators in a multiple linear regression model.  Olemiss Edu.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Deepest-Regression-Estimator","page":"Algorithms","title":"Deepest Regression Estimator","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.deepestregression","category":"page"},{"location":"algorithms/#LinRegOutliers.DeepestRegression.deepestregression","page":"Algorithms","title":"LinRegOutliers.DeepestRegression.deepestregression","text":"deepestregression(setting; maxit = 1000)\n\nEstimate Deepest Regression paramaters.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\nmaxit: Maximum number of iterations\n\nDescription\n\nEstimates Deepest Regression Estimator coefficients.\n\nReferences\n\nVan Aelst S., Rousseeuw P.J., Hubert M., Struyf A. (2002). The deepest regression method. Journal of Multivariate Analysis, 81, 138-166.\n\nOutput\n\nbetas: Vector of regression coefficients estimated.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Robust-Hat-Matrix-based-Initial-Subset-Regressor","page":"Algorithms","title":"Robust Hat Matrix based Initial Subset Regressor","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"LinRegOutliers.robhatreg","category":"page"},{"location":"algorithms/#LinRegOutliers.RobustHatRegression.robhatreg","page":"Algorithms","title":"LinRegOutliers.RobustHatRegression.robhatreg","text":"robhatreg(setting::RegressionSetting)\n\nPerform robust regression using the robust hat matrix method.\n\nArguments\n\nsetting::RegressionSetting: The regression setting.\n\nReturns\n\nA dictionary containing the following\nbetas::AbstractVector: The estimated coefficients.\n\nReferences\n\nSatman, Mehmet Hakan. \"A Robust Initial Basic Subset Selection Method for  Outlier Detection Algorithms in Linear Regression.\"  Journal of Statistics and Applied Sciences 10 (2024): 76-85. https://doi.org/10.52693/jsas.1512794\n\n\n\n\n\nrobhatreg(X, y)\n\nPerform robust regression using the robust hat matrix method.\n\nArguments\n\nX::AbstractMatrix: The design matrix.\ny::AbstractVector: The response vector.\n\nReturns\n\nA dictionary containing the following\nbetas::AbstractVector: The estimated coefficients.\n\nReferences\n\nSatman, Mehmet Hakan. \"A Robust Initial Basic Subset Selection Method for  Outlier Detection Algorithms in Linear Regression.\"  Journal of Statistics and Applied Sciences 10 (2024): 76-85. https://doi.org/10.52693/jsas.1512794\n\n\n\n\n\n","category":"function"}]
}
